{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# other\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "\n",
    "#Env Perso\n",
    "df = pd.read_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\df_movies_cleaned.csv\")\n",
    "\n",
    "# Env Vinci\n",
    "#df = pd.read_csv(r\"C:\\Users\\melvin.derouk\\Desktop\\Data formation\\Movies-Recommandations\\df_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable \"Synopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alors considérée comme illégale, «La Course à la Mort» se pratique toujours dans une prison fédérale. Après une attaque manquée contre le légendaire pilote Frankenstein, Connor Gibson, membre d’une un\n",
      "ité d’élite doit infiltrer la prison avec un objectif : stopper «La Course à la Mort». Il devra alors apprendre se battre dans un monde sans foi ni loi…\n"
     ]
    }
   ],
   "source": [
    "doc = df.Synopsis.sample(1)\n",
    "doc = doc.values[0]\n",
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alors considérée comme illégale, «la course à la mort» se pratique toujours dans une prison fédérale. après une attaque manquée contre le légendaire pilote frankenstein, connor gibson, membre d’une un\n",
      "ité d’élite doit infiltrer la prison avec un objectif : stopper «la course à la mort». il devra alors apprendre se battre dans un monde sans foi ni loi…\n"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alors',\n",
       " 'considérée',\n",
       " 'comme',\n",
       " 'illégale',\n",
       " ',',\n",
       " '«',\n",
       " 'la',\n",
       " 'course',\n",
       " 'à',\n",
       " 'la',\n",
       " 'mort',\n",
       " '»',\n",
       " 'se',\n",
       " 'pratique',\n",
       " 'toujours',\n",
       " 'dans',\n",
       " 'une',\n",
       " 'prison',\n",
       " 'fédérale',\n",
       " '.',\n",
       " 'après',\n",
       " 'une',\n",
       " 'attaque',\n",
       " 'manquée',\n",
       " 'contre',\n",
       " 'le',\n",
       " 'légendaire',\n",
       " 'pilote',\n",
       " 'frankenstein',\n",
       " ',',\n",
       " 'connor',\n",
       " 'gibson',\n",
       " ',',\n",
       " 'membre',\n",
       " 'd',\n",
       " '’',\n",
       " 'une',\n",
       " 'unité',\n",
       " 'd',\n",
       " '’',\n",
       " 'élite',\n",
       " 'doit',\n",
       " 'infiltrer',\n",
       " 'la',\n",
       " 'prison',\n",
       " 'avec',\n",
       " 'un',\n",
       " 'objectif',\n",
       " ':',\n",
       " 'stopper',\n",
       " '«',\n",
       " 'la',\n",
       " 'course',\n",
       " 'à',\n",
       " 'la',\n",
       " 'mort',\n",
       " '»',\n",
       " '.',\n",
       " 'il',\n",
       " 'devra',\n",
       " 'alors',\n",
       " 'apprendre',\n",
       " 'se',\n",
       " 'battre',\n",
       " 'dans',\n",
       " 'un',\n",
       " 'monde',\n",
       " 'sans',\n",
       " 'foi',\n",
       " 'ni',\n",
       " 'loi…']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 71, nb tokens uniques 52\n",
      "['alors', 'considérée', 'comme', 'illégale', ',', '«', 'la', 'course', 'à', 'la', 'mort', '»', 'se', 'pratique', 'toujours', 'dans', 'une', 'prison', 'fédérale', '.', 'après', 'une', 'attaque', 'manquée', 'contre', 'le', 'légendaire', 'pilote', 'frankenstein', ',']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 50, nb tokens uniques 42\n",
      "['alors', 'considérée', 'comme', 'illégale', ',', '«', 'course', 'mort', '»', 'pratique', 'toujours', 'prison', 'fédérale', '.', 'après', 'attaque', 'manquée', 'contre', 'légendaire', 'pilote', 'frankenstein', ',', 'connor', 'gibson', ',', 'membre', '’', 'unité', '’', 'élite']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 59, nb tokens uniques 44\n",
      "['alors', 'considérée', 'comme', 'illégale', 'la', 'course', 'à', 'la', 'mort', 'se', 'pratique', 'toujours', 'dans', 'une', 'prison', 'fédérale', 'après', 'une', 'attaque', 'manquée', 'contre', 'le', 'légendaire', 'pilote', 'frankenstein', 'connor', 'gibson', 'membre', 'd', 'une']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 38, nb tokens uniques 34\n",
      "['alors', 'considérée', 'comme', 'illégale', 'course', 'mort', 'pratique', 'toujours', 'prison', 'fédérale', 'après', 'attaque', 'manquée', 'contre', 'légendaire', 'pilote', 'frankenstein', 'connor', 'gibson', 'membre', 'unité', 'élite', 'doit', 'infiltrer', 'prison', 'objectif', 'stopper', 'course', 'mort', 'devra']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_1(doc, rejoin=False):\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 38, nb tokens uniques 34\n",
      "['alors', 'considérée', 'comme', 'illégale', 'course', 'mort', 'pratique', 'toujours', 'prison', 'fédérale', 'après', 'attaque', 'manquée', 'contre', 'légendaire', 'pilote', 'frankenstein', 'connor', 'gibson', 'membre', 'unité', 'élite', 'doit', 'infiltrer', 'prison', 'objectif', 'stopper', 'course', 'mort', 'devra']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_synopsis_1(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un groupe d\\'animaux animatroniques interprète des chansons pour enfants le jour et fait des razzias meurtrières la nuit. Adaptation du jeu vidéo \"Five Nights at Freddy\\'s\", au croisement du Survival Horror - action - stratégie.Dans l\\'espoir d\\'une guérison miraculeuse, John Kramer se rend au Mexique p'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = \"\".join(df.Synopsis.values)\n",
    "raw_corpus[:3_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3691703"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 365558, nb tokens uniques 37098\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'razzias', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'nights', 'at', 'freddy', 'croisement', 'survival', 'horror', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_1(raw_corpus)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a             3663\n",
       "plus          3111\n",
       "jeune         2002\n",
       "vie           1878\n",
       "alors         1744\n",
       "              ... \n",
       "donbass          1\n",
       "springwood       1\n",
       "veines           1\n",
       "mykola           1\n",
       "tweedledum       1\n",
       "Length: 37098, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a          3663\n",
       "plus       3111\n",
       "jeune      2002\n",
       "vie        1878\n",
       "alors      1744\n",
       "deux       1644\n",
       "tout       1608\n",
       "va         1519\n",
       "après      1338\n",
       "faire      1286\n",
       "monde      1246\n",
       "ans        1212\n",
       "femme      1184\n",
       "fait       1182\n",
       "cette      1179\n",
       "où         1169\n",
       "leurs      1157\n",
       "être       1079\n",
       "famille    1013\n",
       "homme       966\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caractérisée     1\n",
       "arborescence     1\n",
       "trempant         1\n",
       "intermittence    1\n",
       "doublé           1\n",
       "donbass          1\n",
       "springwood       1\n",
       "veines           1\n",
       "mykola           1\n",
       "tweedledum       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37098.000000\n",
       "mean         9.853847\n",
       "std         52.490944\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max       3663.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cochise           1\n",
       "rookery           1\n",
       "videuse           1\n",
       "claremont         1\n",
       "1593              1\n",
       "madelyn           1\n",
       "appellera         1\n",
       "auditoires        1\n",
       "remportent        1\n",
       "morningside       1\n",
       "dévoilé           1\n",
       "mécène            1\n",
       "elsinore          1\n",
       "croyance          1\n",
       "significations    1\n",
       "flingue           1\n",
       "régional          1\n",
       "1845              1\n",
       "bouman            1\n",
       "meurtries         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words = usefull ?\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16119"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cochise',\n",
       " 'rookery',\n",
       " 'videuse',\n",
       " 'claremont',\n",
       " '1593',\n",
       " 'madelyn',\n",
       " 'appellera',\n",
       " 'auditoires',\n",
       " 'remportent',\n",
       " 'morningside',\n",
       " 'dévoilé',\n",
       " 'mécène',\n",
       " 'elsinore',\n",
       " 'croyance',\n",
       " 'significations',\n",
       " 'flingue',\n",
       " 'régional',\n",
       " '1845',\n",
       " 'bouman',\n",
       " 'meurtries']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(\"unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "travaillait      5\n",
       "barnes           5\n",
       "mountain         5\n",
       "balade           5\n",
       "combatif         5\n",
       "omnitrix         5\n",
       "neal             5\n",
       "terrorisé        5\n",
       "rafe             5\n",
       "agatha           5\n",
       "1935             5\n",
       "héroïnes         5\n",
       "krank            5\n",
       "creusent         5\n",
       "psychose         5\n",
       "irréductibles    5\n",
       "nolan            5\n",
       "cotton           5\n",
       "souci            5\n",
       "mordre           5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp==5]\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['travaillait',\n",
       " 'barnes',\n",
       " 'mountain',\n",
       " 'balade',\n",
       " 'combatif',\n",
       " 'omnitrix',\n",
       " 'neal',\n",
       " 'terrorisé',\n",
       " 'rafe',\n",
       " 'agatha',\n",
       " '1935',\n",
       " 'héroïnes',\n",
       " 'krank',\n",
       " 'creusent',\n",
       " 'psychose',\n",
       " 'irréductibles',\n",
       " 'nolan',\n",
       " 'cotton',\n",
       " 'souci',\n",
       " 'mordre']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_min_5_words = list(list_min_5_words.index)\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_min_5_words})\n",
    "tmp.to_csv(\"min_5_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Second cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_2(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \"\"\"cf process_synopsis_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "\n",
    "    positionnal arguments :\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt args : \n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum lenght of a word to not exclude\n",
    "    force_is_alpha : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 365558, nb tokens uniques 37098\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'razzias', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'nights', 'at', 'freddy', 'croisement', 'survival', 'horror', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37098"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 337118, nb tokens uniques 20558\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'freddy', 'croisement', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique', 'procédure', 'médicale', 'risquée', 'expérimentale', 'découvrir']\n"
     ]
    }
   ],
   "source": [
    "#3-5 min process\n",
    "\n",
    "corpus = process_synopsis_2(raw_corpus,\n",
    "                            list_rare_words=list_unique_words,\n",
    "                            rejoin=False)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20558"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stemming & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test fonction stemming\n",
    "\n",
    "def process_synopsis_3(doc,\n",
    "                       rejoin=False,\n",
    "                       lemm_or_stemm=\"stem\",\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens]\n",
    "    else : \n",
    "        trans = FrenchStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test fonction lemmatize\n",
    "\n",
    "def process_synopsis_3(doc,\n",
    "                       rejoin=False,\n",
    "                       lemm_or_stemm=\"lem\",\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = nlp(' '.join(alpha_tokens))\n",
    "        trans_text = [trans.lemma_(i) for i in alpha_tokens]\n",
    "    else : \n",
    "        trans = FrenchStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111898          âgé\n",
       "64680          beth\n",
       "286018         élev\n",
       "223217         frer\n",
       "256193         fait\n",
       "184762        jusqu\n",
       "40748        diabol\n",
       "7410           afro\n",
       "158223      affront\n",
       "179457         leur\n",
       "130774    inexpliqu\n",
       "248714     maternel\n",
       "129850       urbain\n",
       "256529        réfug\n",
       "223716        celui\n",
       "119565      mission\n",
       "38764      familial\n",
       "301796        éclat\n",
       "29897       spécial\n",
       "157323      pensent\n",
       "dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_synopsis_3(raw_corpus, rejoin=False, list_rare_words=list_unique_words)\n",
    "pd.Series(corpus).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12874"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\Preprocessing.ipynb Cell 65\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wordcloud \u001b[39m=\u001b[39m WordCloud(width \u001b[39m=\u001b[39;49m \u001b[39m800\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                       height \u001b[39m=\u001b[39;49m \u001b[39m400\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                       background_color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                       stopwords\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                       max_words\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\u001b[39m.\u001b[39;49mgenerate(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(corpus))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(wordcloud, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Preprocessing.ipynb#Y121sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m     \u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_text(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(words)\n\u001b[0;32m    622\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(\u001b[39mdict\u001b[39;49m(frequencies[:\u001b[39m2\u001b[39;49m]),\n\u001b[0;32m    454\u001b[0m                                    max_font_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight)\n\u001b[0;32m    455\u001b[0m     \u001b[39m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout_]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    505\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m    507\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    509\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    510\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    511\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(font, ImageFont\u001b[39m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly supported for TrueType fonts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m embedded_color \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(width = 800,\n",
    "                      height = 400,\n",
    "                      background_color='white',\n",
    "                      stopwords=[],\n",
    "                      max_words=50).generate(\" \".join(corpus))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(doc) :\n",
    "\n",
    "    new_doc = process_synopsis_3(doc,\n",
    "                                 rejoin=False,\n",
    "                                 lemm_or_stemm=\"stem\",\n",
    "                                 list_rare_words=list_unique_words,\n",
    "                                 min_len_word=3,\n",
    "                                 force_is_alpha=True)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_synopsis'] = df.Synopsis.apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reconverti la colonne clean_synopsis_str en chaine de char\n",
    "df['clean_synopsis_str'] = df['clean_synopsis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Titre</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Date de sortie</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Note</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>clean_synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507089</td>\n",
       "      <td>Five Nights at Freddy's</td>\n",
       "      <td>['Horreur', 'Mystère']</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>Un groupe d'animaux animatroniques interprète ...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>226</td>\n",
       "      <td>[group, animal, animatron, interpret, chanson,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>951491</td>\n",
       "      <td>Saw X</td>\n",
       "      <td>['Horreur', 'Thriller']</td>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>Dans l'espoir d'une guérison miraculeuse, John...</td>\n",
       "      <td>7.4</td>\n",
       "      <td>373</td>\n",
       "      <td>[espoir, guérison, miracul, john, kram, rend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>939335</td>\n",
       "      <td>Muzzle</td>\n",
       "      <td>['Action', 'Crime', 'Drame', 'Thriller']</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>Jake Rosser, officier de police dans l'équipe ...</td>\n",
       "      <td>6.3</td>\n",
       "      <td>272</td>\n",
       "      <td>[jak, offici, polic, équip, los, angel, voit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354912</td>\n",
       "      <td>Coco</td>\n",
       "      <td>['Familial', 'Animation', 'Fantastique', 'Musi...</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>Depuis déjà plusieurs générations, la musique ...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>627</td>\n",
       "      <td>[depuis, déjà, plusieur, géner, musiqu, famill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968051</td>\n",
       "      <td>La Nonne : La Malédiction de Sainte-Lucie</td>\n",
       "      <td>['Horreur', 'Mystère', 'Thriller']</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>En France, en 1956, un prêtre est assassiné da...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>343</td>\n",
       "      <td>[franc, prêtr, assassin, internat, apres, mort...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                      Titre  \\\n",
       "0  507089                    Five Nights at Freddy's   \n",
       "1  951491                                      Saw X   \n",
       "2  939335                                     Muzzle   \n",
       "3  354912                                       Coco   \n",
       "4  968051  La Nonne : La Malédiction de Sainte-Lucie   \n",
       "\n",
       "                                               Genre Date de sortie  \\\n",
       "0                             ['Horreur', 'Mystère']     2023-10-25   \n",
       "1                            ['Horreur', 'Thriller']     2023-09-26   \n",
       "2           ['Action', 'Crime', 'Drame', 'Thriller']     2023-09-29   \n",
       "3  ['Familial', 'Animation', 'Fantastique', 'Musi...     2017-10-27   \n",
       "4                 ['Horreur', 'Mystère', 'Thriller']     2023-09-06   \n",
       "\n",
       "                                            Synopsis  Note  len_txt  \\\n",
       "0  Un groupe d'animaux animatroniques interprète ...   8.4      226   \n",
       "1  Dans l'espoir d'une guérison miraculeuse, John...   7.4      373   \n",
       "2  Jake Rosser, officier de police dans l'équipe ...   6.3      272   \n",
       "3  Depuis déjà plusieurs générations, la musique ...   8.2      627   \n",
       "4  En France, en 1956, un prêtre est assassiné da...   7.0      343   \n",
       "\n",
       "                                      clean_synopsis  \n",
       "0  [group, animal, animatron, interpret, chanson,...  \n",
       "1  [espoir, guérison, miracul, john, kram, rend, ...  \n",
       "2  [jak, offici, polic, équip, los, angel, voit, ...  \n",
       "3  [depuis, déjà, plusieur, géner, musiqu, famill...  \n",
       "4  [franc, prêtr, assassin, internat, apres, mort...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Titre</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Date de sortie</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Note</th>\n",
       "      <th>len_txt</th>\n",
       "      <th>clean_synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>36362</td>\n",
       "      <td>Le Roi des rois</td>\n",
       "      <td>['Drame', 'Fantastique']</td>\n",
       "      <td>1961-10-11</td>\n",
       "      <td>Péplum retraçant la vie de Jésus.</td>\n",
       "      <td>7.1</td>\n",
       "      <td>33</td>\n",
       "      <td>[retrac, vi, jésus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6306</th>\n",
       "      <td>347183</td>\n",
       "      <td>Haikyu !! - Film 1 - Un début et une fin</td>\n",
       "      <td>['Animation', 'Comédie', 'Drame']</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>Premier Haikyuu !! film récapitulatif. Ld</td>\n",
       "      <td>8.5</td>\n",
       "      <td>41</td>\n",
       "      <td>[premi, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>44363</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>['Thriller']</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>Trois skieurs sont bloqués sur un télésiège...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>46</td>\n",
       "      <td>[trois, bloqu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>876716</td>\n",
       "      <td>Ciao Alberto</td>\n",
       "      <td>['Animation', 'Comédie', 'Familial', 'Fantasti...</td>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>Spin-off de Luca, des studios d’animation Pixar.</td>\n",
       "      <td>7.5</td>\n",
       "      <td>48</td>\n",
       "      <td>[spin, off, luc, studios, anim, pixar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>336445</td>\n",
       "      <td>Army of One</td>\n",
       "      <td>['Comédie']</td>\n",
       "      <td>2016-11-04</td>\n",
       "      <td>Un homme décide de traquer seul Oussama Ben La...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>50</td>\n",
       "      <td>[homm, décid, traqu, seul, oussam, ben, laden]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5060</th>\n",
       "      <td>52212</td>\n",
       "      <td>Miranda</td>\n",
       "      <td>['Comédie', 'Romance']</td>\n",
       "      <td>1985-10-15</td>\n",
       "      <td>Miranda cherche un mari et essaye plusieurs ho...</td>\n",
       "      <td>5.2</td>\n",
       "      <td>51</td>\n",
       "      <td>[mirand, cherch, mar, essay, plusieur, homm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>706972</td>\n",
       "      <td>Narco Sub</td>\n",
       "      <td>['Action']</td>\n",
       "      <td>2021-01-22</td>\n",
       "      <td>Un homme deviendra un criminel pour sauver sa ...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>54</td>\n",
       "      <td>[homm, deviendr, criminel, sauv, famill]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6476</th>\n",
       "      <td>286554</td>\n",
       "      <td>Aventure d'un soir</td>\n",
       "      <td>['Romance', 'Comédie']</td>\n",
       "      <td>2014-09-26</td>\n",
       "      <td>Un couple se retrouve pris au piège lors d'une...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>55</td>\n",
       "      <td>[coupl, retrouv, pris, pieg, lor, tempêt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>184341</td>\n",
       "      <td>Hands of Stone</td>\n",
       "      <td>['Drame']</td>\n",
       "      <td>2016-08-26</td>\n",
       "      <td>Retour sur la carrière du boxeur panaméen Robe...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>56</td>\n",
       "      <td>[retour, carri, boxeur, panaméen, roberto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>12110</td>\n",
       "      <td>Dracula, mort et heureux de l’être</td>\n",
       "      <td>['Comédie', 'Horreur']</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>Le \"Dracula\" de Bram Stoker revu et corrigé pa...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59</td>\n",
       "      <td>[dracul, bram, stok, revu, mel, brook]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                     Titre  \\\n",
       "3945   36362                           Le Roi des rois   \n",
       "6306  347183  Haikyu !! - Film 1 - Un début et une fin   \n",
       "7591   44363                                    Frozen   \n",
       "4992  876716                              Ciao Alberto   \n",
       "5395  336445                               Army of One   \n",
       "5060   52212                                   Miranda   \n",
       "6039  706972                                 Narco Sub   \n",
       "6476  286554                        Aventure d'un soir   \n",
       "6066  184341                            Hands of Stone   \n",
       "2632   12110        Dracula, mort et heureux de l’être   \n",
       "\n",
       "                                                  Genre Date de sortie  \\\n",
       "3945                           ['Drame', 'Fantastique']     1961-10-11   \n",
       "6306                  ['Animation', 'Comédie', 'Drame']     2015-07-03   \n",
       "7591                                       ['Thriller']     2010-02-05   \n",
       "4992  ['Animation', 'Comédie', 'Familial', 'Fantasti...     2021-11-12   \n",
       "5395                                        ['Comédie']     2016-11-04   \n",
       "5060                             ['Comédie', 'Romance']     1985-10-15   \n",
       "6039                                         ['Action']     2021-01-22   \n",
       "6476                             ['Romance', 'Comédie']     2014-09-26   \n",
       "6066                                          ['Drame']     2016-08-26   \n",
       "2632                             ['Comédie', 'Horreur']     1995-12-22   \n",
       "\n",
       "                                               Synopsis  Note  len_txt  \\\n",
       "3945                  Péplum retraçant la vie de Jésus.   7.1       33   \n",
       "6306          Premier Haikyuu !! film récapitulatif. Ld   8.5       41   \n",
       "7591     Trois skieurs sont bloqués sur un télésiège...   6.0       46   \n",
       "4992   Spin-off de Luca, des studios d’animation Pixar.   7.5       48   \n",
       "5395  Un homme décide de traquer seul Oussama Ben La...   5.2       50   \n",
       "5060  Miranda cherche un mari et essaye plusieurs ho...   5.2       51   \n",
       "6039  Un homme deviendra un criminel pour sauver sa ...   6.6       54   \n",
       "6476  Un couple se retrouve pris au piège lors d'une...   6.4       55   \n",
       "6066  Retour sur la carrière du boxeur panaméen Robe...   6.5       56   \n",
       "2632  Le \"Dracula\" de Bram Stoker revu et corrigé pa...   6.0       59   \n",
       "\n",
       "                                      clean_synopsis  \n",
       "3945                             [retrac, vi, jésus]  \n",
       "6306                                   [premi, film]  \n",
       "7591                                  [trois, bloqu]  \n",
       "4992          [spin, off, luc, studios, anim, pixar]  \n",
       "5395  [homm, décid, traqu, seul, oussam, ben, laden]  \n",
       "5060    [mirand, cherch, mar, essay, plusieur, homm]  \n",
       "6039        [homm, deviendr, criminel, sauv, famill]  \n",
       "6476       [coupl, retrouv, pris, pieg, lor, tempêt]  \n",
       "6066      [retour, carri, boxeur, panaméen, roberto]  \n",
       "2632          [dracul, bram, stok, revu, mel, brook]  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"len_txt\").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Date de sortie to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date de sortie'] = pd.to_datetime(df['Date de sortie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Age of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.now().year\n",
    "df['Age du film'] = current_year - df['Date de sortie'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Binary encoding of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8893\n",
      "8893\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8893\n"
     ]
    }
   ],
   "source": [
    "df['Genre'] = df['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df['Genre'])\n",
    "\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop('Genre', axis=1), genre_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_movies_preprocess.csv\", index=False)\n",
    "genre_df.to_csv(\"genres_binarized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
