{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# other\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\melvin.derouk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\melvin.derouk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\melvin.derouk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\melvin.derouk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "\n",
    "#Env Perso\n",
    "#df = pd.read_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\df_movies_cleaned.csv\")\n",
    "\n",
    "# Env Vinci\n",
    "df = pd.read_csv(r\"C:\\Users\\melvin.derouk\\Desktop\\Data formation\\Movies-Recommandations\\df_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rowena, une fillette de 11 ans pleine d'entrain, est confrontée au divorce difficile de ses parents. Espérant que les choses redeviennent comme elles étaient avant que la nouvelle petite amie de son p\n",
      "ère et son fils n'entrent en scène, Rowena fait un vœu au Père Noël d'un centre commercial. Cependant, ses souhaits de Noël tournent mal quand elle se retrouve à vivre le même jour encore et encore. C\n",
      "oincée dans cette boucle sans fin, elle doit apprendre à aimer sa nouvelle famille recomposée et apprendre le vrai sens de Noël.\n"
     ]
    }
   ],
   "source": [
    "doc = df.Synopsis.sample(1)\n",
    "doc = doc.values[0]\n",
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rowena, une fillette de 11 ans pleine d'entrain, est confrontée au divorce difficile de ses parents. espérant que les choses redeviennent comme elles étaient avant que la nouvelle petite amie de son p\n",
      "ère et son fils n'entrent en scène, rowena fait un vœu au père noël d'un centre commercial. cependant, ses souhaits de noël tournent mal quand elle se retrouve à vivre le même jour encore et encore. c\n",
      "oincée dans cette boucle sans fin, elle doit apprendre à aimer sa nouvelle famille recomposée et apprendre le vrai sens de noël.\n"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rowena',\n",
       " ',',\n",
       " 'une',\n",
       " 'fillette',\n",
       " 'de',\n",
       " '11',\n",
       " 'ans',\n",
       " 'pleine',\n",
       " \"d'entrain\",\n",
       " ',',\n",
       " 'est',\n",
       " 'confrontée',\n",
       " 'au',\n",
       " 'divorce',\n",
       " 'difficile',\n",
       " 'de',\n",
       " 'ses',\n",
       " 'parents',\n",
       " '.',\n",
       " 'espérant',\n",
       " 'que',\n",
       " 'les',\n",
       " 'choses',\n",
       " 'redeviennent',\n",
       " 'comme',\n",
       " 'elles',\n",
       " 'étaient',\n",
       " 'avant',\n",
       " 'que',\n",
       " 'la',\n",
       " 'nouvelle',\n",
       " 'petite',\n",
       " 'amie',\n",
       " 'de',\n",
       " 'son',\n",
       " 'père',\n",
       " 'et',\n",
       " 'son',\n",
       " 'fils',\n",
       " \"n'entrent\",\n",
       " 'en',\n",
       " 'scène',\n",
       " ',',\n",
       " 'rowena',\n",
       " 'fait',\n",
       " 'un',\n",
       " 'vœu',\n",
       " 'au',\n",
       " 'père',\n",
       " 'noël',\n",
       " \"d'un\",\n",
       " 'centre',\n",
       " 'commercial',\n",
       " '.',\n",
       " 'cependant',\n",
       " ',',\n",
       " 'ses',\n",
       " 'souhaits',\n",
       " 'de',\n",
       " 'noël',\n",
       " 'tournent',\n",
       " 'mal',\n",
       " 'quand',\n",
       " 'elle',\n",
       " 'se',\n",
       " 'retrouve',\n",
       " 'à',\n",
       " 'vivre',\n",
       " 'le',\n",
       " 'même',\n",
       " 'jour',\n",
       " 'encore',\n",
       " 'et',\n",
       " 'encore',\n",
       " '.',\n",
       " 'coincée',\n",
       " 'dans',\n",
       " 'cette',\n",
       " 'boucle',\n",
       " 'sans',\n",
       " 'fin',\n",
       " ',',\n",
       " 'elle',\n",
       " 'doit',\n",
       " 'apprendre',\n",
       " 'à',\n",
       " 'aimer',\n",
       " 'sa',\n",
       " 'nouvelle',\n",
       " 'famille',\n",
       " 'recomposée',\n",
       " 'et',\n",
       " 'apprendre',\n",
       " 'le',\n",
       " 'vrai',\n",
       " 'sens',\n",
       " 'de',\n",
       " 'noël',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 105, nb tokens uniques 74\n",
      "['rowena', ',', 'une', 'fillette', 'de', '11', 'ans', 'pleine', 'd', \"'\", 'entrain', ',', 'est', 'confrontée', 'au', 'divorce', 'difficile', 'de', 'ses', 'parents', '.', 'espérant', 'que', 'les', 'choses', 'redeviennent', 'comme', 'elles', 'étaient', 'avant']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 68, nb tokens uniques 52\n",
      "['rowena', ',', 'fillette', '11', 'ans', 'pleine', \"'\", 'entrain', ',', 'confrontée', 'divorce', 'difficile', 'parents', '.', 'espérant', 'choses', 'redeviennent', 'comme', 'elles', 'avant', 'nouvelle', 'petite', 'amie', 'père', 'fils', \"'\", 'entrent', 'scène', ',', 'rowena']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 93, nb tokens uniques 71\n",
      "['rowena', 'une', 'fillette', 'de', '11', 'ans', 'pleine', 'd', 'entrain', 'est', 'confrontée', 'au', 'divorce', 'difficile', 'de', 'ses', 'parents', 'espérant', 'que', 'les', 'choses', 'redeviennent', 'comme', 'elles', 'étaient', 'avant', 'que', 'la', 'nouvelle', 'petite']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 56, nb tokens uniques 49\n",
      "['rowena', 'fillette', '11', 'ans', 'pleine', 'entrain', 'confrontée', 'divorce', 'difficile', 'parents', 'espérant', 'choses', 'redeviennent', 'comme', 'elles', 'avant', 'nouvelle', 'petite', 'amie', 'père', 'fils', 'entrent', 'scène', 'rowena', 'fait', 'vœu', 'père', 'noël', 'centre', 'commercial']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_1(doc, rejoin=False):\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 56, nb tokens uniques 49\n",
      "['rowena', 'fillette', '11', 'ans', 'pleine', 'entrain', 'confrontée', 'divorce', 'difficile', 'parents', 'espérant', 'choses', 'redeviennent', 'comme', 'elles', 'avant', 'nouvelle', 'petite', 'amie', 'père', 'fils', 'entrent', 'scène', 'rowena', 'fait', 'vœu', 'père', 'noël', 'centre', 'commercial']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_synopsis_1(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un groupe d\\'animaux animatroniques interprète des chansons pour enfants le jour et fait des razzias meurtrières la nuit. Adaptation du jeu vidéo \"Five Nights at Freddy\\'s\", au croisement du Survival Horror - action - stratégie.Dans l\\'espoir d\\'une guérison miraculeuse, John Kramer se rend au Mexique p'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = \"\".join(df.Synopsis.values)\n",
    "raw_corpus[:3_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3707250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 367137, nb tokens uniques 37200\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'razzias', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'nights', 'at', 'freddy', 'croisement', 'survival', 'horror', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_1(raw_corpus)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a             3681\n",
       "plus          3130\n",
       "jeune         2010\n",
       "vie           1888\n",
       "alors         1751\n",
       "              ... \n",
       "medecine         1\n",
       "petillante       1\n",
       "complicite       1\n",
       "immediate        1\n",
       "bohême           1\n",
       "Length: 37200, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a          3681\n",
       "plus       3130\n",
       "jeune      2010\n",
       "vie        1888\n",
       "alors      1751\n",
       "deux       1649\n",
       "tout       1617\n",
       "va         1528\n",
       "après      1351\n",
       "faire      1291\n",
       "monde      1248\n",
       "ans        1221\n",
       "femme      1191\n",
       "fait       1186\n",
       "cette      1180\n",
       "où         1172\n",
       "leurs      1161\n",
       "être       1086\n",
       "famille    1019\n",
       "homme       971\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indissolublement    1\n",
       "inéxorablement      1\n",
       "mure                1\n",
       "jawbreaker          1\n",
       "jullie              1\n",
       "medecine            1\n",
       "petillante          1\n",
       "complicite          1\n",
       "immediate           1\n",
       "bohême              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37200.000000\n",
       "mean         9.869274\n",
       "std         52.656774\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max       3681.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beery           1\n",
       "tilson          1\n",
       "hynde           1\n",
       "sautant         1\n",
       "brentley        1\n",
       "mobil           1\n",
       "anticipée       1\n",
       "ringo           1\n",
       "pampa           1\n",
       "vacancier       1\n",
       "existentiels    1\n",
       "railroad        1\n",
       "tigers          1\n",
       "shandong        1\n",
       "morse           1\n",
       "yasar           1\n",
       "whelan          1\n",
       "fixation        1\n",
       "stratford       1\n",
       "witherspoon     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words = usefull ?\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16176"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beery',\n",
       " 'tilson',\n",
       " 'hynde',\n",
       " 'sautant',\n",
       " 'brentley',\n",
       " 'mobil',\n",
       " 'anticipée',\n",
       " 'ringo',\n",
       " 'pampa',\n",
       " 'vacancier',\n",
       " 'existentiels',\n",
       " 'railroad',\n",
       " 'tigers',\n",
       " 'shandong',\n",
       " 'morse',\n",
       " 'yasar',\n",
       " 'whelan',\n",
       " 'fixation',\n",
       " 'stratford',\n",
       " 'witherspoon']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(\"unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "débarquement     5\n",
       "apte             5\n",
       "capulet          5\n",
       "redonne          5\n",
       "wyoming          5\n",
       "manquer          5\n",
       "raoul            5\n",
       "soigneusement    5\n",
       "ramenés          5\n",
       "jardins          5\n",
       "peterson         5\n",
       "maternité        5\n",
       "ku               5\n",
       "désirent         5\n",
       "piller           5\n",
       "festive          5\n",
       "adrian           5\n",
       "inquiétude       5\n",
       "chambouler       5\n",
       "ferroviaire      5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp==5]\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['débarquement',\n",
       " 'apte',\n",
       " 'capulet',\n",
       " 'redonne',\n",
       " 'wyoming',\n",
       " 'manquer',\n",
       " 'raoul',\n",
       " 'soigneusement',\n",
       " 'ramenés',\n",
       " 'jardins',\n",
       " 'peterson',\n",
       " 'maternité',\n",
       " 'ku',\n",
       " 'désirent',\n",
       " 'piller',\n",
       " 'festive',\n",
       " 'adrian',\n",
       " 'inquiétude',\n",
       " 'chambouler',\n",
       " 'ferroviaire']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_min_5_words = list(list_min_5_words.index)\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_min_5_words})\n",
    "tmp.to_csv(\"min_5_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "déchirée        10\n",
       "cambrioleurs    10\n",
       "financières     10\n",
       "arriveront      10\n",
       "racines         10\n",
       "jeter           10\n",
       "partagé         10\n",
       "voue            10\n",
       "balboa          10\n",
       "prostitution    10\n",
       "wells           10\n",
       "changeant       10\n",
       "accumulent      10\n",
       "traîneau        10\n",
       "mae             10\n",
       "ennuyeuse       10\n",
       "défier          10\n",
       "projeté         10\n",
       "futures         10\n",
       "inexpliqués     10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 10 times ???????????????????\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_10_words = tmp[tmp==10]\n",
    "list_min_10_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Second cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_2(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \"\"\"cf process_synopsis_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "\n",
    "    positionnal arguments :\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt args : \n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum lenght of a word to not exclude\n",
    "    force_is_alpha : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 367137, nb tokens uniques 37200\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'razzias', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'nights', 'at', 'freddy', 'croisement', 'survival', 'horror', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 338579, nb tokens uniques 20602\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'freddy', 'croisement', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique', 'procédure', 'médicale', 'risquée', 'expérimentale', 'découvrir']\n"
     ]
    }
   ],
   "source": [
    "#3-5 min process\n",
    "\n",
    "corpus = process_synopsis_2(raw_corpus,\n",
    "                            list_rare_words=list_unique_words,\n",
    "                            rejoin=False)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20602"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group',\n",
       " 'animal',\n",
       " 'animatron',\n",
       " 'interpret',\n",
       " 'chanson',\n",
       " 'enfant',\n",
       " 'jour',\n",
       " 'fait',\n",
       " 'meurtri',\n",
       " 'nuit',\n",
       " 'adapt',\n",
       " 'jeu',\n",
       " 'vidéo',\n",
       " 'fiv',\n",
       " 'freddy',\n",
       " 'crois',\n",
       " 'action',\n",
       " 'strateg',\n",
       " 'espoir',\n",
       " 'guérison',\n",
       " 'miracul',\n",
       " 'john',\n",
       " 'kram',\n",
       " 'rend',\n",
       " 'mexiqu',\n",
       " 'procédur',\n",
       " 'médical',\n",
       " 'risqu',\n",
       " 'expérimental',\n",
       " 'découvr',\n",
       " 'tout',\n",
       " 'oper',\n",
       " 'arnaqu',\n",
       " 'vis',\n",
       " 'escroqu',\n",
       " 'plus',\n",
       " 'vulner',\n",
       " 'armé',\n",
       " 'nouvel',\n",
       " 'object',\n",
       " 'trist',\n",
       " 'célebr',\n",
       " 'tueur',\n",
       " 'ser',\n",
       " 'utilis',\n",
       " 'pieg',\n",
       " 'ingéni',\n",
       " 'renvers',\n",
       " 'situat',\n",
       " 'contr',\n",
       " 'escroc',\n",
       " 'jak',\n",
       " 'offici',\n",
       " 'polic',\n",
       " 'équip',\n",
       " 'los',\n",
       " 'angel',\n",
       " 'voit',\n",
       " 'partenair',\n",
       " 'dévou',\n",
       " 'mour',\n",
       " 'abattu',\n",
       " 'agresseur',\n",
       " 'alor',\n",
       " 'recherch',\n",
       " 'ident',\n",
       " 'tireur',\n",
       " 'découvr',\n",
       " 'vast',\n",
       " 'réseau',\n",
       " 'flic',\n",
       " 'corrompus',\n",
       " 'impos',\n",
       " 'loi',\n",
       " 'tout',\n",
       " 'vill',\n",
       " 'depuis',\n",
       " 'déjà',\n",
       " 'plusieur',\n",
       " 'géner',\n",
       " 'musiqu',\n",
       " 'famill',\n",
       " 'miguel',\n",
       " 'vrai',\n",
       " 'déchir',\n",
       " 'jeun',\n",
       " 'garçon',\n",
       " 'dont',\n",
       " 'rêv',\n",
       " 'ultim',\n",
       " 'deven',\n",
       " 'musicien',\n",
       " 'auss',\n",
       " 'accompl',\n",
       " 'idol',\n",
       " 'ernesto',\n",
       " 'cruz',\n",
       " 'bien',\n",
       " 'décid',\n",
       " 'prouv',\n",
       " 'talent',\n",
       " 'miguel',\n",
       " 'étrang',\n",
       " 'concour',\n",
       " 'circonst',\n",
       " 'retrouv',\n",
       " 'propuls',\n",
       " 'mond',\n",
       " 'éton',\n",
       " 'color',\n",
       " 'li',\n",
       " 'célebr',\n",
       " 'ancêtr',\n",
       " 'li',\n",
       " 'amiti',\n",
       " 'hector',\n",
       " 'gentil',\n",
       " 'garçon',\n",
       " 'peu',\n",
       " 'filou',\n",
       " 'bord',\n",
       " 'ensembl',\n",
       " 'vont',\n",
       " 'accompl',\n",
       " 'voyag',\n",
       " 'extraordinair',\n",
       " 'rével',\n",
       " 'vérit',\n",
       " 'histoir',\n",
       " 'cach',\n",
       " 'derri',\n",
       " 'cel',\n",
       " 'famill',\n",
       " 'miguel',\n",
       " 'franc',\n",
       " 'prêtr',\n",
       " 'assassin',\n",
       " 'internat',\n",
       " 'apres',\n",
       " 'mort',\n",
       " 'prêtr',\n",
       " 'pass',\n",
       " 'chos',\n",
       " 'peuvent',\n",
       " 'plus',\n",
       " 'être',\n",
       " 'sœur',\n",
       " 'iren',\n",
       " 'nouvel',\n",
       " 'fois',\n",
       " 'confront',\n",
       " 'pouvoir',\n",
       " 'démoniaqu',\n",
       " 'rend',\n",
       " 'vit',\n",
       " 'compt',\n",
       " 'démon',\n",
       " 'retour',\n",
       " 'prépar',\n",
       " 'méfait',\n",
       " 'meurtri',\n",
       " 'depuis',\n",
       " 'femm',\n",
       " 'perdu',\n",
       " 'vi',\n",
       " 'douz',\n",
       " 'an',\n",
       " 'plus',\n",
       " 'tôt',\n",
       " 'victor',\n",
       " 'fielding',\n",
       " 'élev',\n",
       " 'seul',\n",
       " 'fill',\n",
       " 'angel',\n",
       " 'jour',\n",
       " 'angel',\n",
       " 'ami',\n",
       " 'katherin',\n",
       " 'disparaissent',\n",
       " 'bois',\n",
       " 'avant',\n",
       " 'refair',\n",
       " 'surfac',\n",
       " 'heur',\n",
       " 'plus',\n",
       " 'tard',\n",
       " 'san',\n",
       " 'moindr',\n",
       " 'souven',\n",
       " 'arriv',\n",
       " 'des',\n",
       " 'lor',\n",
       " 'étrang',\n",
       " 'éven',\n",
       " 'enchaînent',\n",
       " 'victor',\n",
       " 'doit',\n",
       " 'affront',\n",
       " 'forc',\n",
       " 'malef',\n",
       " 'désesper',\n",
       " 'terroris',\n",
       " 'sollicit',\n",
       " 'seul',\n",
       " 'person',\n",
       " 'encor',\n",
       " 'vi',\n",
       " 'jam',\n",
       " 'témoin',\n",
       " 'phénomen',\n",
       " 'chris',\n",
       " 'homm',\n",
       " 'affair',\n",
       " 'découvr',\n",
       " 'bomb',\n",
       " 'plac',\n",
       " 'voitur',\n",
       " 'conduit',\n",
       " 'assaill',\n",
       " 'inconnu',\n",
       " 'derni',\n",
       " 'ordon',\n",
       " 'exécut',\n",
       " 'ser',\n",
       " 'action',\n",
       " 'tout',\n",
       " 'long',\n",
       " 'journ',\n",
       " 'bomb',\n",
       " 'explos',\n",
       " 'tu',\n",
       " 'famill',\n",
       " 'apres',\n",
       " 'bien',\n",
       " 'mission',\n",
       " 'contr',\n",
       " 'tout',\n",
       " 'attent',\n",
       " 'dom',\n",
       " 'toretto',\n",
       " 'famill',\n",
       " 'déjou',\n",
       " 'surpass',\n",
       " 'tous',\n",
       " 'adversair',\n",
       " 'crois',\n",
       " 'rout',\n",
       " 'aujourd',\n",
       " 'hui',\n",
       " 'fac',\n",
       " 'ennem',\n",
       " 'plus',\n",
       " 'terrifi',\n",
       " 'plus',\n",
       " 'intim',\n",
       " 'brum',\n",
       " 'pass',\n",
       " 'reven',\n",
       " 'assoiff',\n",
       " 'vengeanc',\n",
       " 'bien',\n",
       " 'détermin',\n",
       " 'décim',\n",
       " 'famill',\n",
       " 'réduis',\n",
       " 'né',\n",
       " 'tout',\n",
       " 'quoi',\n",
       " 'surtout',\n",
       " 'dom',\n",
       " 'jam',\n",
       " 'tenu',\n",
       " 'depuis',\n",
       " 'renonc',\n",
       " 'vi',\n",
       " 'assassin',\n",
       " 'servic',\n",
       " 'gouvern',\n",
       " 'robert',\n",
       " 'mccall',\n",
       " 'pein',\n",
       " 'enterr',\n",
       " 'démon',\n",
       " 'pass',\n",
       " 'trouv',\n",
       " 'étrang',\n",
       " 'réconfort',\n",
       " 'défend',\n",
       " 'opprim',\n",
       " 'alor',\n",
       " 'pens',\n",
       " 'avoir',\n",
       " 'trouv',\n",
       " 'havr',\n",
       " 'paix',\n",
       " 'sud',\n",
       " 'ital',\n",
       " 'découvr',\n",
       " 'amis',\n",
       " 'sous',\n",
       " 'contrôl',\n",
       " 'mafi',\n",
       " 'local',\n",
       " 'quand',\n",
       " 'éven',\n",
       " 'prennent',\n",
       " 'tournur',\n",
       " 'mortel',\n",
       " 'mccall',\n",
       " 'sait',\n",
       " 'exact',\n",
       " 'doit',\n",
       " 'fair',\n",
       " 'proteg',\n",
       " 'amis',\n",
       " 'attaqu',\n",
       " 'direct',\n",
       " 'pegr',\n",
       " 'armé',\n",
       " 'tout',\n",
       " 'arme',\n",
       " 'dont',\n",
       " 'peuvent',\n",
       " 'dispos',\n",
       " 'expend',\n",
       " 'derni',\n",
       " 'lign',\n",
       " 'défens',\n",
       " 'mond',\n",
       " 'équip',\n",
       " 'appel',\n",
       " 'lorsqu',\n",
       " 'tout',\n",
       " 'autr',\n",
       " 'option',\n",
       " 'plus',\n",
       " 'possibl',\n",
       " 'mercenair',\n",
       " 'barney',\n",
       " 'ross',\n",
       " 'équip',\n",
       " 'affrontent',\n",
       " 'marchand',\n",
       " 'arme',\n",
       " 'armé',\n",
       " 'priv',\n",
       " 'lorsqu',\n",
       " 'météorit',\n",
       " 'magiqu',\n",
       " 'écras',\n",
       " 'aventurevill',\n",
       " 'don',\n",
       " 'pat',\n",
       " 'patrouill',\n",
       " 'superpouvoir',\n",
       " 'transform',\n",
       " 'sup',\n",
       " 'patrouill',\n",
       " 'chos',\n",
       " 'dégen',\n",
       " 'lorsqu',\n",
       " 'monsieur',\n",
       " 'helling',\n",
       " 'ennem',\n",
       " 'jur',\n",
       " 'amis',\n",
       " 'évad',\n",
       " 'prison',\n",
       " 'assoc',\n",
       " 'scientif',\n",
       " 'foll',\n",
       " 'afin',\n",
       " 'vol',\n",
       " 'nouveau',\n",
       " 'pouvoir',\n",
       " 'destin',\n",
       " 'aventurevill',\n",
       " 'désorm',\n",
       " 'jeu',\n",
       " 'sup',\n",
       " 'patrouill',\n",
       " 'doit',\n",
       " 'arrêt',\n",
       " 'sup',\n",
       " 'mech',\n",
       " 'avant',\n",
       " 'trop',\n",
       " 'tard',\n",
       " 'vill',\n",
       " 'difficil',\n",
       " 'mississipp',\n",
       " 'femm',\n",
       " 'jeun',\n",
       " 'fill',\n",
       " 'pris',\n",
       " 'entre',\n",
       " 'deux',\n",
       " 'feux',\n",
       " 'lorsqu',\n",
       " 'whisky',\n",
       " 'arme',\n",
       " 'feu',\n",
       " 'des',\n",
       " 'vengeanc',\n",
       " 'croisent',\n",
       " 'violent',\n",
       " 'ethan',\n",
       " 'hunt',\n",
       " 'équip',\n",
       " 'imf',\n",
       " 'lancent',\n",
       " 'mission',\n",
       " 'plus',\n",
       " 'périll',\n",
       " 'jour',\n",
       " 'traqu',\n",
       " 'effroi',\n",
       " 'nouvel',\n",
       " 'arme',\n",
       " 'avant',\n",
       " 'cel',\n",
       " 'tomb',\n",
       " 'entre',\n",
       " 'mauvais',\n",
       " 'main',\n",
       " 'menac',\n",
       " 'human',\n",
       " 'entier',\n",
       " 'contrôl',\n",
       " 'futur',\n",
       " 'destin',\n",
       " 'mond',\n",
       " 'jeu',\n",
       " 'alor',\n",
       " 'forc',\n",
       " 'obscur',\n",
       " 'pass',\n",
       " 'ressurg',\n",
       " 'ethan',\n",
       " 'engag',\n",
       " 'cours',\n",
       " 'mortel',\n",
       " 'autour',\n",
       " 'glob',\n",
       " 'confront',\n",
       " 'puiss',\n",
       " 'énigmat',\n",
       " 'ennem',\n",
       " 'ethan',\n",
       " 'réalis',\n",
       " 'rien',\n",
       " 'peut',\n",
       " 'plac',\n",
       " 'dessus',\n",
       " 'mission',\n",
       " 'vi',\n",
       " 'ceux',\n",
       " 'aim',\n",
       " 'gran',\n",
       " 'turismo',\n",
       " 'inspir',\n",
       " 'incroi',\n",
       " 'histoir',\n",
       " 'vécu',\n",
       " 'invraisembl',\n",
       " 'band',\n",
       " 'amateur',\n",
       " 'jeu',\n",
       " 'vidéo',\n",
       " 'class',\n",
       " 'ouvri',\n",
       " 'pilot',\n",
       " 'cours',\n",
       " 'paum',\n",
       " 'idéal',\n",
       " 'industr',\n",
       " 'sport',\n",
       " 'ensembl',\n",
       " 'risquent',\n",
       " 'tout',\n",
       " 'perc',\n",
       " 'sport',\n",
       " 'plus',\n",
       " 'prestigi',\n",
       " 'mond',\n",
       " 'gran',\n",
       " 'turismo',\n",
       " 'histoir',\n",
       " 'feu',\n",
       " 'roul',\n",
       " 'action',\n",
       " 'prouvent',\n",
       " 'tout',\n",
       " 'possibl',\n",
       " 'lorsqu',\n",
       " 'conduit',\n",
       " 'énerg',\n",
       " 'désespoir',\n",
       " 'lorsqu',\n",
       " 'cellul',\n",
       " 'terror',\n",
       " 'international',\n",
       " 'menac',\n",
       " 'stabl',\n",
       " 'polit',\n",
       " 'mondial',\n",
       " 'enlev',\n",
       " 'collègu',\n",
       " 'agent',\n",
       " 'tireur',\n",
       " 'élit',\n",
       " 'brandon',\n",
       " 'becket',\n",
       " 'nouvel',\n",
       " 'brigad',\n",
       " 'secret',\n",
       " 'men',\n",
       " 'colonel',\n",
       " 'ston',\n",
       " 'doit',\n",
       " 'franch',\n",
       " 'continent',\n",
       " 'jusqu',\n",
       " 'infiltr',\n",
       " 'cellul',\n",
       " 'abattr',\n",
       " 'lead',\n",
       " 'afin',\n",
       " 'liber',\n",
       " 'lady',\n",
       " 'death',\n",
       " 'mettr',\n",
       " 'fin',\n",
       " 'menac',\n",
       " 'ryan',\n",
       " 'robbin',\n",
       " 'josh',\n",
       " 'égal',\n",
       " 'vedet',\n",
       " 'début',\n",
       " 'glob',\n",
       " 'franchis',\n",
       " 'snip',\n",
       " 'apres',\n",
       " 'avoir',\n",
       " 'sauv',\n",
       " 'jeun',\n",
       " 'garçon',\n",
       " 'impitoi',\n",
       " 'trafiqu',\n",
       " 'enfant',\n",
       " 'agent',\n",
       " 'fédéral',\n",
       " 'apprend',\n",
       " 'sœur',\n",
       " 'garçon',\n",
       " 'toujour',\n",
       " 'captiv',\n",
       " 'décid',\n",
       " 'sauv',\n",
       " 'auss',\n",
       " 'delà',\n",
       " 'haut',\n",
       " 'montagn',\n",
       " 'noir',\n",
       " 'cach',\n",
       " 'royaum',\n",
       " 'peupl',\n",
       " 'créatur',\n",
       " 'fantast',\n",
       " 'depuis',\n",
       " 'siecl',\n",
       " 'elle',\n",
       " 'protègent',\n",
       " 'mond',\n",
       " 'homm',\n",
       " 'sourc',\n",
       " 'vi',\n",
       " 'éternel',\n",
       " 'pouvoir',\n",
       " 'infin',\n",
       " 'jusqu',\n",
       " 'jour',\n",
       " 'nouvel',\n",
       " 'élu',\n",
       " 'cet',\n",
       " 'forêt',\n",
       " 'enchant',\n",
       " 'rencontr',\n",
       " 'luc',\n",
       " 'jeun',\n",
       " 'humain',\n",
       " 'égar',\n",
       " 'montagn',\n",
       " 'encontr',\n",
       " 'regl',\n",
       " 'depuis',\n",
       " 'millénair',\n",
       " 'vont',\n",
       " 'revoir',\n",
       " 'san',\n",
       " 'prendr',\n",
       " 'gard',\n",
       " 'conséquent',\n",
       " 'royaum',\n",
       " 'aventur',\n",
       " 'fait',\n",
       " 'commenc',\n",
       " 'équip',\n",
       " 'chercheur',\n",
       " 'part',\n",
       " 'explor',\n",
       " 'profondeur',\n",
       " 'océan',\n",
       " 'péripl',\n",
       " 'tourn',\n",
       " 'catastroph',\n",
       " 'lorsqu',\n",
       " 'oper',\n",
       " 'extract',\n",
       " 'mini',\n",
       " 'illégal',\n",
       " 'met',\n",
       " 'péril',\n",
       " 'mission',\n",
       " 'vi',\n",
       " 'confront',\n",
       " 'immens',\n",
       " 'bandit',\n",
       " 'san',\n",
       " 'piti',\n",
       " 'héros',\n",
       " 'doivent',\n",
       " 'échapp',\n",
       " 'terribl',\n",
       " 'prédateur',\n",
       " 'gard',\n",
       " 'toujour',\n",
       " 'temp',\n",
       " 'avanc',\n",
       " 'terrifi',\n",
       " 'cours',\n",
       " 'contr',\n",
       " 'montr',\n",
       " 'singh',\n",
       " 'armé',\n",
       " 'indien',\n",
       " 'mission',\n",
       " 'marqu',\n",
       " 'histoir',\n",
       " 'inde',\n",
       " 'fraîch',\n",
       " 'diplôm',\n",
       " 'univers',\n",
       " 'jaim',\n",
       " 'rey',\n",
       " 'rentr',\n",
       " 'chez',\n",
       " 'plein',\n",
       " 'ambit',\n",
       " 'découvr',\n",
       " 'situat',\n",
       " 'bien',\n",
       " 'chang',\n",
       " 'depuis',\n",
       " 'départ',\n",
       " 'tand',\n",
       " 'cherch',\n",
       " 'plac',\n",
       " 'mond',\n",
       " 'destin',\n",
       " 'mêl',\n",
       " 'jaim',\n",
       " 'retrouv',\n",
       " 'hasard',\n",
       " 'possess',\n",
       " 'scarab',\n",
       " 'ancien',\n",
       " 'reliqu',\n",
       " 'biotechnolog',\n",
       " 'extraterrestr',\n",
       " 'des',\n",
       " 'lor',\n",
       " 'scarab',\n",
       " 'chois',\n",
       " 'fair',\n",
       " 'jaim',\n",
       " 'hôt',\n",
       " 'jeun',\n",
       " 'homm',\n",
       " 'voit',\n",
       " 'revêtu',\n",
       " 'armur',\n",
       " 'hor',\n",
       " 'commun',\n",
       " 'octroi',\n",
       " 'pouvoir',\n",
       " 'extraordinair',\n",
       " 'imprévisibl',\n",
       " 'tout',\n",
       " 'bascul',\n",
       " 'alor',\n",
       " 'jaim',\n",
       " 'devient',\n",
       " 'sup',\n",
       " 'héros',\n",
       " 'blu',\n",
       " 'beetl',\n",
       " 'lorsqu',\n",
       " 'group',\n",
       " 'amis',\n",
       " 'découvr',\n",
       " 'comment',\n",
       " 'conjur',\n",
       " 'esprit',\n",
       " 'aid',\n",
       " 'mystéri',\n",
       " 'main',\n",
       " 'hant',\n",
       " 'deviennent',\n",
       " 'accros',\n",
       " 'nouveau',\n",
       " 'frisson',\n",
       " 'expérient',\n",
       " 'fait',\n",
       " 'tour',\n",
       " 'réseau',\n",
       " 'social',\n",
       " 'seul',\n",
       " 'regl',\n",
       " 'respect',\n",
       " 'doivent',\n",
       " 'ten',\n",
       " 'main',\n",
       " 'plus',\n",
       " 'second',\n",
       " 'lorsqu',\n",
       " 'entre',\n",
       " 'enfreint',\n",
       " 'vont',\n",
       " 'être',\n",
       " 'rattrap',\n",
       " 'esprit',\n",
       " 'oblig',\n",
       " 'chois',\n",
       " 'fi',\n",
       " 'mort',\n",
       " 'viv',\n",
       " 'tous',\n",
       " 'personnag',\n",
       " 'préfer',\n",
       " 'retrouvent',\n",
       " 'fois',\n",
       " 'studio',\n",
       " 'moment',\n",
       " 'festif',\n",
       " 'amus',\n",
       " 'émouv',\n",
       " 'alor',\n",
       " 'rassemblent',\n",
       " 'photo',\n",
       " 'group',\n",
       " 'spectaculair',\n",
       " 'afin',\n",
       " 'célebr',\n",
       " 'an',\n",
       " 'disney',\n",
       " 'superb',\n",
       " 'plui',\n",
       " 'fait',\n",
       " 'objet',\n",
       " 'tout',\n",
       " 'attent',\n",
       " 'gen',\n",
       " 'déplac',\n",
       " 'mass',\n",
       " 'assist',\n",
       " 'spectacl',\n",
       " 'alor',\n",
       " 'ciel',\n",
       " 'illumin',\n",
       " 'lumin',\n",
       " 'plui',\n",
       " 'devient',\n",
       " 'final',\n",
       " 'tempêt',\n",
       " 'meurtri',\n",
       " 'alor',\n",
       " 'debr',\n",
       " 'commencent',\n",
       " 'nombreux',\n",
       " 'dégât',\n",
       " 'fair',\n",
       " 'quelqu',\n",
       " 'victim',\n",
       " 'vill',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'menac',\n",
       " 'pend',\n",
       " 'autor',\n",
       " 'tentent',\n",
       " 'évacu',\n",
       " 'popul',\n",
       " 'calm',\n",
       " 'scientif',\n",
       " 'découvrent',\n",
       " 'rest',\n",
       " 'ancien',\n",
       " 'météorit',\n",
       " 'enfou',\n",
       " 'depuis',\n",
       " 'longtemp',\n",
       " 'sous',\n",
       " 'vill',\n",
       " 'exercent',\n",
       " 'attract',\n",
       " 'magnet',\n",
       " 'corp',\n",
       " 'célest',\n",
       " 'autr',\n",
       " 'problem',\n",
       " 'ajout',\n",
       " 'situat',\n",
       " 'déjà',\n",
       " 'catastroph',\n",
       " 'météor',\n",
       " 'grand',\n",
       " 'taill',\n",
       " 'dirig',\n",
       " 'égal',\n",
       " 'ver',\n",
       " 'terr',\n",
       " 'apres',\n",
       " 'second',\n",
       " 'guerr',\n",
       " 'mondial',\n",
       " 'sombr',\n",
       " 'veil',\n",
       " 'toussaint',\n",
       " 'retrait',\n",
       " 'viv',\n",
       " 'exil',\n",
       " 'volontair',\n",
       " 'resplend',\n",
       " 'venis',\n",
       " 'ital',\n",
       " 'poirot',\n",
       " 'assist',\n",
       " 'contr',\n",
       " 'cœur',\n",
       " 'séanc',\n",
       " 'spirit',\n",
       " 'hant',\n",
       " 'ruin',\n",
       " 'quand',\n",
       " 'invit',\n",
       " 'assassin',\n",
       " 'détect',\n",
       " 'propuls',\n",
       " 'univer',\n",
       " 'sinistr',\n",
       " 'obscur',\n",
       " 'secret',\n",
       " 'apres',\n",
       " 'avoir',\n",
       " 'retrouv',\n",
       " 'gwen',\n",
       " 'stacy',\n",
       " 'spid',\n",
       " 'man',\n",
       " 'sympath',\n",
       " 'héros',\n",
       " 'originair',\n",
       " 'brooklyn',\n",
       " 'catapult',\n",
       " 'traver',\n",
       " 'multiver',\n",
       " 'rencontr',\n",
       " 'équip',\n",
       " 'spid',\n",
       " 'héros',\n",
       " 'charg',\n",
       " 'proteg',\n",
       " 'existent',\n",
       " 'lorsqu',\n",
       " 'héros',\n",
       " 'opposent',\n",
       " 'façon',\n",
       " 'ger',\n",
       " 'nouvel',\n",
       " 'menac',\n",
       " 'mil',\n",
       " 'retrouv',\n",
       " 'confront',\n",
       " 'doit',\n",
       " 'redéfin',\n",
       " 'signif',\n",
       " 'être',\n",
       " 'héros',\n",
       " 'afin',\n",
       " 'sauv',\n",
       " 'person',\n",
       " 'aim',\n",
       " 'plus',\n",
       " 'alor',\n",
       " 'autobot',\n",
       " 'viennent',\n",
       " 'arriv',\n",
       " 'optimus',\n",
       " 'prim',\n",
       " 'terr',\n",
       " 'archéologu',\n",
       " 'ancien',\n",
       " 'militair',\n",
       " 'vont',\n",
       " 'découvr',\n",
       " 'trac',\n",
       " 'guerr',\n",
       " 'entre',\n",
       " 'deux',\n",
       " 'ancien',\n",
       " 'faction',\n",
       " 'transformer',\n",
       " 'maximal',\n",
       " 'predacon',\n",
       " 'optimus',\n",
       " 'prim',\n",
       " 'devr',\n",
       " 'fair',\n",
       " 'fac',\n",
       " 'nouveau',\n",
       " 'ennem',\n",
       " 'predacon',\n",
       " 'redout',\n",
       " 'chef',\n",
       " 'autobot',\n",
       " 'maximal',\n",
       " 'feront',\n",
       " 'fac',\n",
       " 'menac',\n",
       " 'encor',\n",
       " 'plus',\n",
       " 'grand',\n",
       " 'unicron',\n",
       " 'planet',\n",
       " 'métall',\n",
       " 'gé',\n",
       " 'apprêt',\n",
       " 'dévor',\n",
       " 'terr',\n",
       " 'parallel',\n",
       " 'mond',\n",
       " 'réel',\n",
       " 'exist',\n",
       " 'barb',\n",
       " 'land',\n",
       " 'mond',\n",
       " 'parf',\n",
       " 'poup',\n",
       " 'barb',\n",
       " 'vivent',\n",
       " 'joyeux',\n",
       " 'avoir',\n",
       " 'rendu',\n",
       " 'fill',\n",
       " 'humain',\n",
       " 'heureux',\n",
       " 'jour',\n",
       " 'barb',\n",
       " 'commenc',\n",
       " 'pos',\n",
       " 'question',\n",
       " 'deven',\n",
       " 'humain',\n",
       " 'conseil',\n",
       " 'barb',\n",
       " 'étrang',\n",
       " 'part',\n",
       " 'mond',\n",
       " 'réel',\n",
       " 'afin',\n",
       " 'retrouv',\n",
       " 'fill',\n",
       " 'laquel',\n",
       " 'apparten',\n",
       " 'afin',\n",
       " 'pouvoir',\n",
       " 'retrouv',\n",
       " 'perfect',\n",
       " 'quêt',\n",
       " 'accompagn',\n",
       " 'ken',\n",
       " 'fou',\n",
       " 'amour',\n",
       " 'égal',\n",
       " 'trouv',\n",
       " 'sen',\n",
       " 'vi',\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    return [stemmer.stem(word) for word in token_list]\n",
    "\n",
    "stem_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Syno psis'] = df['Synopsis'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour supprimer les pronoms, articles, determinants etc \n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def remove_stop_words(token_list):\n",
    "    return [word for word in token_list if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction à la colonne Synopsis\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une liste de la colonne Synopsis\n",
    "all_words = [word for token_list in df_train['Synopsis'] for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frequencies = Counter(all_words)\n",
    "\n",
    "# Count the frequencies of the remaining words\n",
    "filtered_word_frequencies = Counter(filtered_frequencies)\n",
    "\n",
    "most_common_words = filtered_word_frequencies.most_common(30) \n",
    "\n",
    "# Display the 10 most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiner la liste de stop words à partir des mots les + fréquents ? Genre \"a\", \"plus\", \"où\"...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming \n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    return [stemmer.stem(word) for word in token_list]\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "model = Word2Vec(df_train['Synopsis'], vector_size=300, window=5, min_count=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(df_train['Synopsis'], total_examples=len(df_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('zomb')\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation binaires des genres\n",
    "print(len(df_train))\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Genre'] = df_train['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df_train['Genre'])\n",
    "\n",
    "# Créer un DataFrame avec les résultats\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train.drop('Genre', axis=1), genre_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Création du modèle de recommandation\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la similarité cosinus\n",
    "#cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Synopsis'] = df['Synopsis'].apply(lambda x: ' '.join(x))\n",
    "count_matrix = CountVectorizer().fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df.index, index=df['Titre']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour obtenir des recommandations\n",
    "def get_recommendations(title, cosine_sim, df_movies_md, num_of_recs=10):\n",
    "\n",
    "    title = title.lower()\n",
    "    if title not in df_movies_md['Titre'].str.lower().values:\n",
    "        return f\"Aucune recommandation trouvée pour: {title}\"\n",
    "    \n",
    "    # Obtenir l'indice du film donné son titre\n",
    "    idx = df_movies_md[df_movies_md['Titre'].str.lower() == title.lower()].index[0]\n",
    "\n",
    "    # Obtenir les scores de similarité pour ce film avec tous les films\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Multiplier par la note du film pour chaque score\n",
    "    sim_scores = [(i, score * (df_movies_md.iloc[i]['Note'] / 10)) for i, score in sim_scores]\n",
    "\n",
    "    # Trier les films en fonction des scores calculés\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtenir les scores des n films les plus similaires\n",
    "    sim_scores = sim_scores[1:num_of_recs+1]\n",
    "\n",
    "    # Obtenir les indices de ces films\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Retourner les films correspondants\n",
    "    return df_movies_md.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir des recommandations pour un film donné\n",
    "recommendations = get_recommendations(\"Coco\", cosine_sim, indices)\n",
    "\n",
    "# Afficher les recommandations\n",
    "print(recommendations)\n",
    "df[df['Titre'].str.contains(\"irréversible\", case=False, na=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
