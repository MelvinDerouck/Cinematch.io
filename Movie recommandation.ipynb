{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# other\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "\n",
    "#Env Perso\n",
    "df = pd.read_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\df_movies_cleaned.csv\")\n",
    "\n",
    "# Env Vinci\n",
    "#df = pd.read_csv(r\"C:\\Users\\melvin.derouk\\Desktop\\Data formation\\Movies-Recommandations\\df_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuyant un passé douloureux, Chloé démarre une nouvelle vie d'institutrice dans le Morvan avec son fils Jules, 8 ans. Accueillie chaleureusement par les habitants du village, elle tombe sous le charme \n",
      "de Mathieu, un médecin charismatique et mystérieux.  Mais de terribles événements perturbent la tranquillité des villageois : un enfant a disparu et une bête sauvage s’attaque au bétail. Jules est en \n",
      "alerte, il le sent, quelque chose rôde la nuit autour de la maison...\n"
     ]
    }
   ],
   "source": [
    "doc = df.Synopsis.sample(1)\n",
    "doc = doc.values[0]\n",
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuyant un passé douloureux, chloé démarre une nouvelle vie d'institutrice dans le morvan avec son fils jules, 8 ans. accueillie chaleureusement par les habitants du village, elle tombe sous le charme \n",
      "de mathieu, un médecin charismatique et mystérieux.  mais de terribles événements perturbent la tranquillité des villageois : un enfant a disparu et une bête sauvage s’attaque au bétail. jules est en \n",
      "alerte, il le sent, quelque chose rôde la nuit autour de la maison...\n"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuyant',\n",
       " 'un',\n",
       " 'passé',\n",
       " 'douloureux',\n",
       " ',',\n",
       " 'chloé',\n",
       " 'démarre',\n",
       " 'une',\n",
       " 'nouvelle',\n",
       " 'vie',\n",
       " \"d'institutrice\",\n",
       " 'dans',\n",
       " 'le',\n",
       " 'morvan',\n",
       " 'avec',\n",
       " 'son',\n",
       " 'fils',\n",
       " 'jules',\n",
       " ',',\n",
       " '8',\n",
       " 'ans',\n",
       " '.',\n",
       " 'accueillie',\n",
       " 'chaleureusement',\n",
       " 'par',\n",
       " 'les',\n",
       " 'habitants',\n",
       " 'du',\n",
       " 'village',\n",
       " ',',\n",
       " 'elle',\n",
       " 'tombe',\n",
       " 'sous',\n",
       " 'le',\n",
       " 'charme',\n",
       " 'de',\n",
       " 'mathieu',\n",
       " ',',\n",
       " 'un',\n",
       " 'médecin',\n",
       " 'charismatique',\n",
       " 'et',\n",
       " 'mystérieux',\n",
       " '.',\n",
       " 'mais',\n",
       " 'de',\n",
       " 'terribles',\n",
       " 'événements',\n",
       " 'perturbent',\n",
       " 'la',\n",
       " 'tranquillité',\n",
       " 'des',\n",
       " 'villageois',\n",
       " ':',\n",
       " 'un',\n",
       " 'enfant',\n",
       " 'a',\n",
       " 'disparu',\n",
       " 'et',\n",
       " 'une',\n",
       " 'bête',\n",
       " 'sauvage',\n",
       " 's',\n",
       " '’',\n",
       " 'attaque',\n",
       " 'au',\n",
       " 'bétail',\n",
       " '.',\n",
       " 'jules',\n",
       " 'est',\n",
       " 'en',\n",
       " 'alerte',\n",
       " ',',\n",
       " 'il',\n",
       " 'le',\n",
       " 'sent',\n",
       " ',',\n",
       " 'quelque',\n",
       " 'chose',\n",
       " 'rôde',\n",
       " 'la',\n",
       " 'nuit',\n",
       " 'autour',\n",
       " 'de',\n",
       " 'la',\n",
       " 'maison',\n",
       " '...']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 89, nb tokens uniques 71\n",
      "['fuyant', 'un', 'passé', 'douloureux', ',', 'chloé', 'démarre', 'une', 'nouvelle', 'vie', 'd', \"'\", 'institutrice', 'dans', 'le', 'morvan', 'avec', 'son', 'fils', 'jules', ',', '8', 'ans', '.', 'accueillie', 'chaleureusement', 'par', 'les', 'habitants', 'du']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fbfb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\Movie recommandation.ipynb Cell 24\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Movie%20recommandation.ipynb#Y312sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fbfb\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fbfb' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_1(doc, rejoin=False):\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = process_synopsis_1(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Un groupe d\\'animaux animatroniques interprète des chansons pour enfants le jour et fait des razzias meurtrières la nuit. Adaptation du jeu vidéo \"Five Nights at Freddy\\'s\", au croisement du Survival Horror - action - stratégie.Dans l\\'espoir d\\'une guérison miraculeuse, John Kramer se rend au Mexique p'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = \"\".join(df.Synopsis.values)\n",
    "raw_corpus[:3_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3707250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 367137, nb tokens uniques 37200\n",
      "['groupe', 'animaux', 'animatroniques', 'interprète', 'chansons', 'enfants', 'jour', 'fait', 'razzias', 'meurtrières', 'nuit', 'adaptation', 'jeu', 'vidéo', 'five', 'nights', 'at', 'freddy', 'croisement', 'survival', 'horror', 'action', 'stratégie', 'espoir', 'guérison', 'miraculeuse', 'john', 'kramer', 'rend', 'mexique']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_1(raw_corpus)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a             3681\n",
       "plus          3130\n",
       "jeune         2010\n",
       "vie           1888\n",
       "alors         1751\n",
       "              ... \n",
       "medecine         1\n",
       "petillante       1\n",
       "complicite       1\n",
       "immediate        1\n",
       "bohême           1\n",
       "Length: 37200, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beery           1\n",
       "tilson          1\n",
       "hynde           1\n",
       "sautant         1\n",
       "brentley        1\n",
       "mobil           1\n",
       "anticipée       1\n",
       "ringo           1\n",
       "pampa           1\n",
       "vacancier       1\n",
       "existentiels    1\n",
       "railroad        1\n",
       "tigers          1\n",
       "shandong        1\n",
       "morse           1\n",
       "yasar           1\n",
       "whelan          1\n",
       "fixation        1\n",
       "stratford       1\n",
       "witherspoon     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words = usefull ?\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beery',\n",
       " 'tilson',\n",
       " 'hynde',\n",
       " 'sautant',\n",
       " 'brentley',\n",
       " 'mobil',\n",
       " 'anticipée',\n",
       " 'ringo',\n",
       " 'pampa',\n",
       " 'vacancier',\n",
       " 'existentiels',\n",
       " 'railroad',\n",
       " 'tigers',\n",
       " 'shandong',\n",
       " 'morse',\n",
       " 'yasar',\n",
       " 'whelan',\n",
       " 'fixation',\n",
       " 'stratford',\n",
       " 'witherspoon']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(\"unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp==5]\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_min_5_words = list(list_min_5_words.index)\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_min_5_words})\n",
    "tmp.to_csv(\"min_5_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idem for min 10 times ???????????????????\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_10_words = tmp[tmp==10]\n",
    "list_min_10_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_min_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Second cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_2(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \"\"\"cf process_synopsis_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "\n",
    "    positionnal arguments :\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt args : \n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum lenght of a word to not exclude\n",
    "    force_is_alpha : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-5 min process\n",
    "\n",
    "corpus = process_synopsis_2(raw_corpus,\n",
    "                            list_rare_words=list_unique_words,\n",
    "                            rejoin=False)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stemming & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_3(doc,\n",
    "                       rejoin=False,\n",
    "                       lemm_or_stemm=\"stem\",\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens]\n",
    "    else : \n",
    "        trans = FrenchStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = process_synopsis_3(raw_corpus, rejoin=False, list_rare_words=list_unique_words)\n",
    "pd.Series(corpus).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800,\n",
    "                      height = 400,\n",
    "                      background_color='white',\n",
    "                      stopwords=[],\n",
    "                      font_path=r\"C:\\Users\\derou\\OneDrive\\Bureau\\POLICE\\Sono-Medium.ttf\",\n",
    "                      max_words=50).generate(\" \".join(corpus))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(doc) :\n",
    "\n",
    "    new_doc = process_synopsis_3(doc,\n",
    "                                 rejoin=False,\n",
    "                                 lemm_or_stemm=\"stem\",\n",
    "                                 list_rare_words=list_unique_words,\n",
    "                                 min_len_word=3,\n",
    "                                 force_is_alpha=True)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b869f7dfb84ef5b2e51204df08e9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2240), Label(value='0 / 2240'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['clean_synopsis'] = df.Synopsis.parallel_apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Syno psis'] = df['Synopsis'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour supprimer les pronoms, articles, determinants etc \n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def remove_stop_words(token_list):\n",
    "    return [word for word in token_list if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction à la colonne Synopsis\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une liste de la colonne Synopsis\n",
    "all_words = [word for token_list in df_train['Synopsis'] for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frequencies = Counter(all_words)\n",
    "\n",
    "# Count the frequencies of the remaining words\n",
    "filtered_word_frequencies = Counter(filtered_frequencies)\n",
    "\n",
    "most_common_words = filtered_word_frequencies.most_common(30) \n",
    "\n",
    "# Display the 10 most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiner la liste de stop words à partir des mots les + fréquents ? Genre \"a\", \"plus\", \"où\"...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming \n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    return [stemmer.stem(word) for word in token_list]\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "model = Word2Vec(df_train['Synopsis'], vector_size=300, window=5, min_count=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(df_train['Synopsis'], total_examples=len(df_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('zomb')\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation binaires des genres\n",
    "print(len(df_train))\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Genre'] = df_train['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df_train['Genre'])\n",
    "\n",
    "# Créer un DataFrame avec les résultats\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train.drop('Genre', axis=1), genre_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Création du modèle de recommandation\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la similarité cosinus\n",
    "#cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Synopsis'] = df['Synopsis'].apply(lambda x: ' '.join(x))\n",
    "count_matrix = CountVectorizer().fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df.index, index=df['Titre']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour obtenir des recommandations\n",
    "def get_recommendations(title, cosine_sim, df_movies_md, num_of_recs=10):\n",
    "\n",
    "    title = title.lower()\n",
    "    if title not in df_movies_md['Titre'].str.lower().values:\n",
    "        return f\"Aucune recommandation trouvée pour: {title}\"\n",
    "    \n",
    "    # Obtenir l'indice du film donné son titre\n",
    "    idx = df_movies_md[df_movies_md['Titre'].str.lower() == title.lower()].index[0]\n",
    "\n",
    "    # Obtenir les scores de similarité pour ce film avec tous les films\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Multiplier par la note du film pour chaque score\n",
    "    sim_scores = [(i, score * (df_movies_md.iloc[i]['Note'] / 10)) for i, score in sim_scores]\n",
    "\n",
    "    # Trier les films en fonction des scores calculés\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtenir les scores des n films les plus similaires\n",
    "    sim_scores = sim_scores[1:num_of_recs+1]\n",
    "\n",
    "    # Obtenir les indices de ces films\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Retourner les films correspondants\n",
    "    return df_movies_md.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir des recommandations pour un film donné\n",
    "recommendations = get_recommendations(\"Coco\", cosine_sim, indices)\n",
    "\n",
    "# Afficher les recommandations\n",
    "print(recommendations)\n",
    "df[df['Titre'].str.contains(\"irréversible\", case=False, na=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
