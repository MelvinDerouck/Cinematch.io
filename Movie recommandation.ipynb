{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# other\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "df = pd.read_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\df_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les indices pleuvent et la traque est lancée ! L'affaire du siècle est sur le point d'éclater au grand jour dans Basil, Détective privé. La célèbre souris détective vous emmène dans les rues pavées de\n",
      " Londres en 1897. Un enlèvement suspect est le point de départ de cette palpitante aventure musicale. Olivia, la fille d'un fabricant de jouets londonien, se rend à Baker Street pour demander à Basil \n",
      "de l'aider à retrouver son père. L'assistant de Basil, le docteur Dawson, et Toby le chien fidèle vont lui donner un coup de main, et un coup de truffe, pour chercher des indices dans leur merveilleux\n",
      " monde miniature. La piste les mènera finalement au professeur Ratigan, un criminel sans pitié que Basil va devoir duper pour sauver le royaume des souris !\n"
     ]
    }
   ],
   "source": [
    "random_doc = df.Synopsis.sample(1)\n",
    "random_doc = random_doc.values[0]\n",
    "print(insert_newlines(random_doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_doc = random_doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\Movie recommandation.ipynb Cell 15\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Movie%20recommandation.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(insert_newlines(tokens, every\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\Movie recommandation.ipynb Cell 15\u001b[0m line \u001b[0;36minsert_newlines\u001b[1;34m(string, every)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Movie%20recommandation.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(string), every):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Movie%20recommandation.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lines\u001b[39m.\u001b[39mappend(string[i:i\u001b[39m+\u001b[39mevery])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Movie%20recommandation.ipynb#X62sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(lines)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(random_doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['les',\n",
       " 'indices',\n",
       " 'pleuvent',\n",
       " 'et',\n",
       " 'la',\n",
       " 'traque',\n",
       " 'est',\n",
       " 'lancée',\n",
       " '!',\n",
       " \"l'affaire\",\n",
       " 'du',\n",
       " 'siècle',\n",
       " 'est',\n",
       " 'sur',\n",
       " 'le',\n",
       " 'point',\n",
       " \"d'éclater\",\n",
       " 'au',\n",
       " 'grand',\n",
       " 'jour',\n",
       " 'dans',\n",
       " 'basil',\n",
       " ',',\n",
       " 'détective',\n",
       " 'privé',\n",
       " '.',\n",
       " 'la',\n",
       " 'célèbre',\n",
       " 'souris',\n",
       " 'détective',\n",
       " 'vous',\n",
       " 'emmène',\n",
       " 'dans',\n",
       " 'les',\n",
       " 'rues',\n",
       " 'pavées',\n",
       " 'de',\n",
       " 'londres',\n",
       " 'en',\n",
       " '1897.',\n",
       " 'un',\n",
       " 'enlèvement',\n",
       " 'suspect',\n",
       " 'est',\n",
       " 'le',\n",
       " 'point',\n",
       " 'de',\n",
       " 'départ',\n",
       " 'de',\n",
       " 'cette',\n",
       " 'palpitante',\n",
       " 'aventure',\n",
       " 'musicale',\n",
       " '.',\n",
       " 'olivia',\n",
       " ',',\n",
       " 'la',\n",
       " 'fille',\n",
       " \"d'un\",\n",
       " 'fabricant',\n",
       " 'de',\n",
       " 'jouets',\n",
       " 'londonien',\n",
       " ',',\n",
       " 'se',\n",
       " 'rend',\n",
       " 'à',\n",
       " 'baker',\n",
       " 'street',\n",
       " 'pour',\n",
       " 'demander',\n",
       " 'à',\n",
       " 'basil',\n",
       " 'de',\n",
       " \"l'aider\",\n",
       " 'à',\n",
       " 'retrouver',\n",
       " 'son',\n",
       " 'père',\n",
       " '.',\n",
       " \"l'assistant\",\n",
       " 'de',\n",
       " 'basil',\n",
       " ',',\n",
       " 'le',\n",
       " 'docteur',\n",
       " 'dawson',\n",
       " ',',\n",
       " 'et',\n",
       " 'toby',\n",
       " 'le',\n",
       " 'chien',\n",
       " 'fidèle',\n",
       " 'vont',\n",
       " 'lui',\n",
       " 'donner',\n",
       " 'un',\n",
       " 'coup',\n",
       " 'de',\n",
       " 'main',\n",
       " ',',\n",
       " 'et',\n",
       " 'un',\n",
       " 'coup',\n",
       " 'de',\n",
       " 'truffe',\n",
       " ',',\n",
       " 'pour',\n",
       " 'chercher',\n",
       " 'des',\n",
       " 'indices',\n",
       " 'dans',\n",
       " 'leur',\n",
       " 'merveilleux',\n",
       " 'monde',\n",
       " 'miniature',\n",
       " '.',\n",
       " 'la',\n",
       " 'piste',\n",
       " 'les',\n",
       " 'mènera',\n",
       " 'finalement',\n",
       " 'au',\n",
       " 'professeur',\n",
       " 'ratigan',\n",
       " ',',\n",
       " 'un',\n",
       " 'criminel',\n",
       " 'sans',\n",
       " 'pitié',\n",
       " 'que',\n",
       " 'basil',\n",
       " 'va',\n",
       " 'devoir',\n",
       " 'duper',\n",
       " 'pour',\n",
       " 'sauver',\n",
       " 'le',\n",
       " 'royaume',\n",
       " 'des',\n",
       " 'souris',\n",
       " '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(random_doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 153, nb tokens uniques 94\n",
      "['les', 'indices', 'pleuvent', 'et', 'la', 'traque', 'est', 'lancée', '!', 'l', \"'\", 'affaire', 'du', 'siècle', 'est', 'sur', 'le', 'point', 'd', \"'\", 'éclater', 'au', 'grand', 'jour', 'dans', 'basil', ',', 'détective', 'privé', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(random_doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 95, nb tokens uniques 71\n",
      "['indices', 'pleuvent', 'traque', 'lancée', '!', \"'\", 'affaire', 'siècle', 'point', \"'\", 'éclater', 'grand', 'jour', 'basil', ',', 'détective', 'privé', '.', 'célèbre', 'souris', 'détective', 'emmène', 'rues', 'pavées', 'londres', '1897', '.', 'enlèvement', 'suspect', 'point']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 133, nb tokens uniques 90\n",
      "['les', 'indices', 'pleuvent', 'et', 'la', 'traque', 'est', 'lancée', 'l', 'affaire', 'du', 'siècle', 'est', 'sur', 'le', 'point', 'd', 'éclater', 'au', 'grand', 'jour', 'dans', 'basil', 'détective', 'privé', 'la', 'célèbre', 'souris', 'détective', 'vous']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(random_doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 75, nb tokens uniques 67\n",
      "['indices', 'pleuvent', 'traque', 'lancée', 'affaire', 'siècle', 'point', 'éclater', 'grand', 'jour', 'basil', 'détective', 'privé', 'célèbre', 'souris', 'détective', 'emmène', 'rues', 'pavées', 'londres', '1897', 'enlèvement', 'suspect', 'point', 'départ', 'cette', 'palpitante', 'aventure', 'musicale', 'olivia']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Synopsis'] = df['Synopsis'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction pour supprimer les pronoms, articles, determinants etc \n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def remove_stop_words(token_list):\n",
    "    return [word for word in token_list if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction à la colonne Synopsis\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une liste de la colonne Synopsis\n",
    "all_words = [word for token_list in df_train['Synopsis'] for word in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_frequencies = Counter(all_words)\n",
    "\n",
    "# Count the frequencies of the remaining words\n",
    "filtered_word_frequencies = Counter(filtered_frequencies)\n",
    "\n",
    "most_common_words = filtered_word_frequencies.most_common(30) \n",
    "\n",
    "# Display the 10 most common words\n",
    "print(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affiner la liste de stop words à partir des mots les + fréquents ? Genre \"a\", \"plus\", \"où\"...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming \n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "\n",
    "def stem_words(token_list):\n",
    "    return [stemmer.stem(word) for word in token_list]\n",
    "\n",
    "df_train['Synopsis'] = df_train['Synopsis'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "model = Word2Vec(df_train['Synopsis'], vector_size=300, window=5, min_count=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(df_train['Synopsis'], total_examples=len(df_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = model.wv.most_similar('zomb')\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorisation binaires des genres\n",
    "print(len(df_train))\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Genre'] = df_train['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df_train['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df_train['Genre'])\n",
    "\n",
    "# Créer un DataFrame avec les résultats\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train.drop('Genre', axis=1), genre_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Création du modèle de recommandation\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la similarité cosinus\n",
    "#cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Synopsis'] = df['Synopsis'].apply(lambda x: ' '.join(x))\n",
    "count_matrix = CountVectorizer().fit_transform(df['Synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df.index, index=df['Titre']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour obtenir des recommandations\n",
    "def get_recommendations(title, cosine_sim, df_movies_md, num_of_recs=10):\n",
    "\n",
    "    title = title.lower()\n",
    "    if title not in df_movies_md['Titre'].str.lower().values:\n",
    "        return f\"Aucune recommandation trouvée pour: {title}\"\n",
    "    \n",
    "    # Obtenir l'indice du film donné son titre\n",
    "    idx = df_movies_md[df_movies_md['Titre'].str.lower() == title.lower()].index[0]\n",
    "\n",
    "    # Obtenir les scores de similarité pour ce film avec tous les films\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Multiplier par la note du film pour chaque score\n",
    "    sim_scores = [(i, score * (df_movies_md.iloc[i]['Note'] / 10)) for i, score in sim_scores]\n",
    "\n",
    "    # Trier les films en fonction des scores calculés\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtenir les scores des n films les plus similaires\n",
    "    sim_scores = sim_scores[1:num_of_recs+1]\n",
    "\n",
    "    # Obtenir les indices de ces films\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Retourner les films correspondants\n",
    "    return df_movies_md.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir des recommandations pour un film donné\n",
    "recommendations = get_recommendations(\"Coco\", cosine_sim, indices)\n",
    "\n",
    "# Afficher les recommandations\n",
    "print(recommendations)\n",
    "df[df['Titre'].str.contains(\"irréversible\", case=False, na=False)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
