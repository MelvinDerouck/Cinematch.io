{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# other\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Melvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Melvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Melvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Melvin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire de sauvegarde: C:\\Users\\Melvin\\Desktop\\DATA\\PORTFOLIO\\Recommandation de films\\data\n",
      "\n",
      "Date du jour : 2026-02-10\n"
     ]
    }
   ],
   "source": [
    "sns.set()\n",
    "\n",
    "BASE_DIR = Path().resolve().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "\n",
    "print(f\"Répertoire de sauvegarde: {DATA_DIR}\\n\")\n",
    "\n",
    "today = date.today()\n",
    "print(\"Date du jour :\", today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR / 'data' / 'df_movies_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable \"Synopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin des années 70, une équipe de tournage investit une maison isolée du fin fond du Texas pour y réaliser un film X. À la tombée de la nuit, les propriétaires des lieux surprennent les cinéastes amate\n",
      "urs en plein acte. Le tournage vire brutalement au cauchemar.\n"
     ]
    }
   ],
   "source": [
    "doc = df.Synopsis.sample(1)\n",
    "doc = doc.values[0]\n",
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin des années 70, une équipe de tournage investit une maison isolée du fin fond du texas pour y réaliser un film x. à la tombée de la nuit, les propriétaires des lieux surprennent les cinéastes amate\n",
      "urs en plein acte. le tournage vire brutalement au cauchemar.\n"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fin',\n",
       " 'des',\n",
       " 'années',\n",
       " '70',\n",
       " ',',\n",
       " 'une',\n",
       " 'équipe',\n",
       " 'de',\n",
       " 'tournage',\n",
       " 'investit',\n",
       " 'une',\n",
       " 'maison',\n",
       " 'isolée',\n",
       " 'du',\n",
       " 'fin',\n",
       " 'fond',\n",
       " 'du',\n",
       " 'texas',\n",
       " 'pour',\n",
       " 'y',\n",
       " 'réaliser',\n",
       " 'un',\n",
       " 'film',\n",
       " 'x.',\n",
       " 'à',\n",
       " 'la',\n",
       " 'tombée',\n",
       " 'de',\n",
       " 'la',\n",
       " 'nuit',\n",
       " ',',\n",
       " 'les',\n",
       " 'propriétaires',\n",
       " 'des',\n",
       " 'lieux',\n",
       " 'surprennent',\n",
       " 'les',\n",
       " 'cinéastes',\n",
       " 'amateurs',\n",
       " 'en',\n",
       " 'plein',\n",
       " 'acte',\n",
       " '.',\n",
       " 'le',\n",
       " 'tournage',\n",
       " 'vire',\n",
       " 'brutalement',\n",
       " 'au',\n",
       " 'cauchemar',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 51, nb tokens uniques 40\n",
      "['fin', 'des', 'années', '70', ',', 'une', 'équipe', 'de', 'tournage', 'investit', 'une', 'maison', 'isolée', 'du', 'fin', 'fond', 'du', 'texas', 'pour', 'y', 'réaliser', 'un', 'film', 'x', '.', 'à', 'la', 'tombée', 'de', 'la']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 32, nb tokens uniques 27\n",
      "['fin', 'années', '70', ',', 'équipe', 'tournage', 'investit', 'maison', 'isolée', 'fin', 'fond', 'texas', 'réaliser', 'film', 'x', '.', 'tombée', 'nuit', ',', 'propriétaires', 'lieux', 'surprennent', 'cinéastes', 'amateurs', 'plein', 'acte', '.', 'tournage', 'vire', 'brutalement']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 46, nb tokens uniques 38\n",
      "['fin', 'des', 'années', '70', 'une', 'équipe', 'de', 'tournage', 'investit', 'une', 'maison', 'isolée', 'du', 'fin', 'fond', 'du', 'texas', 'pour', 'y', 'réaliser', 'un', 'film', 'x', 'à', 'la', 'tombée', 'de', 'la', 'nuit', 'les']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 27, nb tokens uniques 25\n",
      "['fin', 'années', '70', 'équipe', 'tournage', 'investit', 'maison', 'isolée', 'fin', 'fond', 'texas', 'réaliser', 'film', 'x', 'tombée', 'nuit', 'propriétaires', 'lieux', 'surprennent', 'cinéastes', 'amateurs', 'plein', 'acte', 'tournage', 'vire', 'brutalement', 'cauchemar']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_1(doc, rejoin=False):\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 27, nb tokens uniques 25\n",
      "['fin', 'années', '70', 'équipe', 'tournage', 'investit', 'maison', 'isolée', 'fin', 'fond', 'texas', 'réaliser', 'film', 'x', 'tombée', 'nuit', 'propriétaires', 'lieux', 'surprennent', 'cinéastes', 'amateurs', 'plein', 'acte', 'tournage', 'vire', 'brutalement', 'cauchemar']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_synopsis_1(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Après l'impact dévastateur d'une comète qui a réduit la Terre en ruines, la famille Garrity doit quitter la sécurité de son bunker au Groenland. Commence alors un périple pour leur survie et l'avenir de l'Humanité à travers un monde dévasté à la recherche d'un nouveau foyer.En quête d’un nouveau dép\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = \"\".join(df.Synopsis.values)\n",
    "raw_corpus[:3_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2419538"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 238572, nb tokens uniques 30320\n",
      "['après', 'impact', 'dévastateur', 'comète', 'a', 'réduit', 'terre', 'ruines', 'famille', 'garrity', 'doit', 'quitter', 'sécurité', 'bunker', 'groenland', 'commence', 'alors', 'périple', 'survie', 'avenir', 'humanité', 'travers', 'monde', 'dévasté', 'recherche', 'nouveau', 'foyer', 'quête', 'nouveau', 'départ']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_1(raw_corpus)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a             2387\n",
       "plus          1993\n",
       "jeune         1379\n",
       "vie           1304\n",
       "alors         1116\n",
       "              ... \n",
       "revanchard       1\n",
       "chul             1\n",
       "mutilée          1\n",
       "arènes           1\n",
       "alexia           1\n",
       "Name: count, Length: 30320, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a        2387\n",
       "plus     1993\n",
       "jeune    1379\n",
       "vie      1304\n",
       "alors    1116\n",
       "deux     1067\n",
       "tout     1005\n",
       "va        989\n",
       "après     850\n",
       "ans       833\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paillard      1\n",
       "fisc          1\n",
       "pitreries     1\n",
       "pelouses      1\n",
       "tondues       1\n",
       "revanchard    1\n",
       "chul          1\n",
       "mutilée       1\n",
       "arènes        1\n",
       "alexia        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30320.000000\n",
       "mean         7.868470\n",
       "std         37.864155\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max       2387.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "immortalise        1\n",
       "émue               1\n",
       "sutra              1\n",
       "numérisés          1\n",
       "néfaste            1\n",
       "collectionne       1\n",
       "conventionnelle    1\n",
       "affectera          1\n",
       "allégorie          1\n",
       "dépouillé          1\n",
       "artificial         1\n",
       "rashida            1\n",
       "morley             1\n",
       "fallacieux         1\n",
       "gorges             1\n",
       "tangible           1\n",
       "rocketbelt         1\n",
       "prostate           1\n",
       "psychologie        1\n",
       "prédestinée        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words = usefull ?\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13680"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['immortalise',\n",
       " 'émue',\n",
       " 'sutra',\n",
       " 'numérisés',\n",
       " 'néfaste',\n",
       " 'collectionne',\n",
       " 'conventionnelle',\n",
       " 'affectera',\n",
       " 'allégorie',\n",
       " 'dépouillé',\n",
       " 'artificial',\n",
       " 'rashida',\n",
       " 'morley',\n",
       " 'fallacieux',\n",
       " 'gorges',\n",
       " 'tangible',\n",
       " 'rocketbelt',\n",
       " 'prostate',\n",
       " 'psychologie',\n",
       " 'prédestinée']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "\n",
    "save_path = DATA_DIR / 'unique_words.csv'\n",
    "tmp.to_csv(save_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mick          5\n",
       "semblait      5\n",
       "audace        5\n",
       "ocean         5\n",
       "matrice       5\n",
       "movie         5\n",
       "étonnantes    5\n",
       "penchant      5\n",
       "capturée      5\n",
       "army          5\n",
       "député        5\n",
       "vlad          5\n",
       "kurt          5\n",
       "jonny         5\n",
       "igor          5\n",
       "associent     5\n",
       "suédois       5\n",
       "étendue       5\n",
       "équipes       5\n",
       "paisibles     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp==5]\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mick',\n",
       " 'semblait',\n",
       " 'audace',\n",
       " 'ocean',\n",
       " 'matrice',\n",
       " 'movie',\n",
       " 'étonnantes',\n",
       " 'penchant',\n",
       " 'capturée',\n",
       " 'army',\n",
       " 'député',\n",
       " 'vlad',\n",
       " 'kurt',\n",
       " 'jonny',\n",
       " 'igor',\n",
       " 'associent',\n",
       " 'suédois',\n",
       " 'étendue',\n",
       " 'équipes',\n",
       " 'paisibles']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_min_5_words = list(list_min_5_words.index)\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_min_5_words})\n",
    "\n",
    "save_path = DATA_DIR / 'min_5_words.csv'\n",
    "tmp.to_csv(save_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Second cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_2(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=2,\n",
    "                       force_is_alpha=\"alpha\") : \n",
    "    \n",
    "    \"\"\"cf process_synopsis_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "\n",
    "    positionnal arguments :\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt args : \n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum lenght of a word to not exclude\n",
    "    force_is_alpha : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha == \"alpha\":\n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    elif force_is_alpha == \"digits4\":\n",
    "        alpha_tokens = [w for w in more_than_N if re.fullmatch(r'\\d{4}', w)]\n",
    "    else:\n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 238572, nb tokens uniques 30320\n",
      "['après', 'impact', 'dévastateur', 'comète', 'a', 'réduit', 'terre', 'ruines', 'famille', 'garrity', 'doit', 'quitter', 'sécurité', 'bunker', 'groenland', 'commence', 'alors', 'périple', 'survie', 'avenir', 'humanité', 'travers', 'monde', 'dévasté', 'recherche', 'nouveau', 'foyer', 'quête', 'nouveau', 'départ']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30320"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 220573, nb tokens uniques 16401\n",
      "['après', 'impact', 'dévastateur', 'comète', 'réduit', 'terre', 'ruines', 'famille', 'garrity', 'doit', 'quitter', 'sécurité', 'bunker', 'groenland', 'commence', 'alors', 'périple', 'survie', 'avenir', 'humanité', 'travers', 'monde', 'dévasté', 'recherche', 'nouveau', 'foyer', 'quête', 'nouveau', 'départ', 'millie']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_2(raw_corpus,\n",
    "                            list_rare_words=list_unique_words,\n",
    "                            rejoin=False)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16401"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1018.78it/s, Materializing param=pooler.dense.weight]                              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_3(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=2,\n",
    "                       force_is_alpha=\"alpha\",\n",
    "                       use_embeddings=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process synopsis with optional embeddings instead of lemmatization/stemming\n",
    "    \n",
    "    Positional arguments:\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "    \n",
    "    Optional args:\n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum length of a word to not exclude\n",
    "    force_is_alpha : if \"alpha\", exclude all tokens with a numeric character\n",
    "    use_embeddings : bool : if True, return embeddings vector instead of tokens\n",
    "    \n",
    "    Return:\n",
    "    ------------------------\n",
    "    - If use_embeddings=True: numpy array (embedding vector)\n",
    "    - If rejoin=True: string (tokens joined)\n",
    "    - Otherwise: list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # List unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "    \n",
    "    # Lower\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "    \n",
    "    # Stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "    \n",
    "    #############################################################\n",
    "    # Filtering\n",
    "    #############################################################\n",
    "    \n",
    "    # No rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "    \n",
    "    # No words shorter than min_len_word\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "    \n",
    "    # Only alpha chars\n",
    "    if force_is_alpha == \"alpha\":\n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    elif force_is_alpha == \"digits4\":\n",
    "        alpha_tokens = [w for w in more_than_N if re.fullmatch(r'\\d{4}', w)]\n",
    "    else:\n",
    "        alpha_tokens = more_than_N\n",
    "    \n",
    "    #############################################################\n",
    "    # Embeddings OR tokens\n",
    "    #############################################################\n",
    "    \n",
    "    if use_embeddings:\n",
    "        # Join tokens into text\n",
    "        clean_text = ' '.join(alpha_tokens)\n",
    "        \n",
    "        # embedding (dense vector capturing meaning)\n",
    "        embedding = embedding_model.encode(clean_text, convert_to_numpy=True)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # Manage return type\n",
    "    if rejoin:\n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création des embeddings pour chaque film...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5958/5958 [02:15<00:00, 44.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "print(\"Création des embeddings pour chaque film...\")\n",
    "\n",
    "df['embeddings'] = df.Synopsis.progress_apply(\n",
    "    lambda x: process_synopsis_3(x, \n",
    "                                 list_rare_words=list_unique_words,\n",
    "                                 min_len_word=2,\n",
    "                                 force_is_alpha=\"alpha\",\n",
    "                                 use_embeddings=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de similarité créée\n"
     ]
    }
   ],
   "source": [
    "# Créez la matrice d'embeddings\n",
    "import numpy as np\n",
    "embeddings_matrix = np.vstack(df['embeddings'].values)\n",
    "\n",
    "# Calculez la similarité\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(embeddings_matrix, embeddings_matrix)\n",
    "\n",
    "print(\"Matrice de similarité créée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. FILMS LES PLUS UNIQUES (faible similarité moyenne)\n",
      "------------------------------------------------------------\n",
      "                             Titre  avg_similarity\n",
      "                           Airlift        0.061157\n",
      "                         Bloodwork        0.074734\n",
      "                             Speed        0.075177\n",
      "American Strike - L'ultime mission        0.076932\n",
      "                Popeye le Plombier        0.079250\n",
      "  American Nightmare 3 : Élections        0.081291\n",
      "                Situation critique        0.094935\n",
      "              Le sable était rouge        0.098110\n",
      "                              Dawn        0.098431\n",
      "                            Piston        0.099015\n",
      "\n",
      "4. FILMS LES PLUS GÉNÉRIQUES (forte similarité moyenne)\n",
      "------------------------------------------------------------\n",
      "                        Titre  avg_similarity\n",
      "                Batman Begins        0.424669\n",
      "                        Cabal        0.414049\n",
      "            The Family Plan 2        0.406590\n",
      "Les Cavaliers de l'Apocalypse        0.406446\n",
      "       L'Exorciste : Dévotion        0.402023\n",
      "              Queen of hearts        0.400652\n",
      "                Quatre frères        0.400267\n",
      "                 Donnie Darko        0.398080\n",
      "       Bons baisers de Bruges        0.396728\n",
      "                  Impitoyable        0.396643\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n3. FILMS LES PLUS UNIQUES (faible similarité moyenne)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculez la similarité moyenne de chaque film avec tous les autres\n",
    "avg_similarities = []\n",
    "for i in range(len(cosine_sim)):\n",
    "    # Similarité moyenne (en excluant le film lui-même)\n",
    "    sim_without_self = np.concatenate([cosine_sim[i][:i], cosine_sim[i][i+1:]])\n",
    "    avg_similarities.append(sim_without_self.mean())\n",
    "\n",
    "df['avg_similarity'] = avg_similarities\n",
    "\n",
    "# Films les plus uniques (faible similarité)\n",
    "unique_films = df.nsmallest(10, 'avg_similarity')[['Titre', 'avg_similarity']]\n",
    "print(unique_films.to_string(index=False))\n",
    "\n",
    "print(\"\\n4. FILMS LES PLUS GÉNÉRIQUES (forte similarité moyenne)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Films les plus génériques (forte similarité)\n",
    "generic_films = df.nlargest(10, 'avg_similarity')[['Titre', 'avg_similarity']]\n",
    "print(generic_films.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(doc) :\n",
    "\n",
    "    new_doc = process_synopsis_3(doc,\n",
    "                                 rejoin=False,\n",
    "                                 list_rare_words=list_unique_words,\n",
    "                                 min_len_word=2,\n",
    "                                 force_is_alpha=\"alpha\")\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fin des années 70, une équipe de tournage investit une maison isolée du fin fond du texas pour y réaliser un film x. à la tombée de la nuit, les propriétaires des lieux surprennent les cinéastes amateurs en plein acte. le tournage vire brutalement au cauchemar.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_synopsis'] = df.Synopsis.apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reconverti la colonne clean_synopsis_str en chaine de char\n",
    "df['clean_synopsis_str'] = df['clean_synopsis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Date de sortie to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date de sortie'] = pd.to_datetime(df['Date de sortie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Age of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.now().year\n",
    "df['Age du film'] = current_year - df['Date de sortie'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Titre</th>\n",
       "      <th>Date de sortie</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Note</th>\n",
       "      <th>Popularité</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Affiche</th>\n",
       "      <th>periode_5_ans</th>\n",
       "      <th>poids_temporel</th>\n",
       "      <th>Note_Catégorie</th>\n",
       "      <th>Pop_Catégorie</th>\n",
       "      <th>Note_Ajustée</th>\n",
       "      <th>Catégorie_Adaptée</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>avg_similarity</th>\n",
       "      <th>clean_synopsis</th>\n",
       "      <th>clean_synopsis_str</th>\n",
       "      <th>Age du film</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>840464</td>\n",
       "      <td>Greenland : Migration</td>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>['Aventure', 'Thriller', 'Science-Fiction']</td>\n",
       "      <td>6.567</td>\n",
       "      <td>499.8774</td>\n",
       "      <td>Après l'impact dévastateur d'une comète qui a ...</td>\n",
       "      <td>/9eaHOemq2Ch5rYPJ0uXcipZUYpS.jpg</td>\n",
       "      <td>2025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Bon (6-7)</td>\n",
       "      <td>Très élevée</td>\n",
       "      <td>6.564427</td>\n",
       "      <td>Extrêmement populaire (&gt;18.1)</td>\n",
       "      <td>[-0.027700055, 0.16916879, 0.018964056, -0.224...</td>\n",
       "      <td>0.256877</td>\n",
       "      <td>[après, impact, dévastateur, comète, réduit, t...</td>\n",
       "      <td>après impact dévastateur comète réduit terre r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1368166</td>\n",
       "      <td>La Femme de ménage</td>\n",
       "      <td>2025-12-18</td>\n",
       "      <td>['Mystère', 'Thriller']</td>\n",
       "      <td>7.178</td>\n",
       "      <td>473.7526</td>\n",
       "      <td>En quête d’un nouveau départ, Millie accepte u...</td>\n",
       "      <td>/28Dn93nuxH8PmDLUVQLCjLNVxKD.jpg</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Très bon (7-8)</td>\n",
       "      <td>Très élevée</td>\n",
       "      <td>7.167667</td>\n",
       "      <td>Extrêmement populaire (&gt;18.1)</td>\n",
       "      <td>[0.10259485, -0.0042940257, -0.1724262, 0.0959...</td>\n",
       "      <td>0.343441</td>\n",
       "      <td>[quête, nouveau, départ, millie, accepte, post...</td>\n",
       "      <td>quête nouveau départ millie accepte poste femm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1168190</td>\n",
       "      <td>Team Démolition</td>\n",
       "      <td>2026-01-28</td>\n",
       "      <td>['Action', 'Comédie', 'Crime', 'Mystère']</td>\n",
       "      <td>6.802</td>\n",
       "      <td>418.4280</td>\n",
       "      <td>Deux demi-frères brouillés, Jonny et James, se...</td>\n",
       "      <td>/ec0bwYUEYBhcHutx4nIShLzX7Dl.jpg</td>\n",
       "      <td>2025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Bon (6-7)</td>\n",
       "      <td>Très élevée</td>\n",
       "      <td>6.795620</td>\n",
       "      <td>Extrêmement populaire (&gt;18.1)</td>\n",
       "      <td>[0.030473026, 0.04308578, -0.07412312, -0.2527...</td>\n",
       "      <td>0.254323</td>\n",
       "      <td>[deux, demi, frères, jonny, james, réunissent,...</td>\n",
       "      <td>deux demi frères jonny james réunissent après ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1084242</td>\n",
       "      <td>Zootopie 2</td>\n",
       "      <td>2025-11-26</td>\n",
       "      <td>['Animation', 'Comédie', 'Aventure', 'Familial...</td>\n",
       "      <td>7.600</td>\n",
       "      <td>290.4141</td>\n",
       "      <td>Judy Hopps et Nick Wilde explorent à nouveau Z...</td>\n",
       "      <td>/hBI7Wrps6tDjhEzBxJgoPLhbmT1.jpg</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Très bon (7-8)</td>\n",
       "      <td>Très élevée</td>\n",
       "      <td>7.574757</td>\n",
       "      <td>Extrêmement populaire (&gt;18.1)</td>\n",
       "      <td>[0.119301155, -0.084989265, 0.032965213, 0.155...</td>\n",
       "      <td>0.309305</td>\n",
       "      <td>[judy, hopps, nick, wilde, explorent, nouveau,...</td>\n",
       "      <td>judy hopps nick wilde explorent nouveau zootop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419406</td>\n",
       "      <td>The Shadow's Edge</td>\n",
       "      <td>2025-08-16</td>\n",
       "      <td>['Action', 'Crime', 'Drame', 'Thriller']</td>\n",
       "      <td>7.202</td>\n",
       "      <td>262.5255</td>\n",
       "      <td>Un mystérieux mafieux et ses 7 fils adoptifs m...</td>\n",
       "      <td>/t1PFVsGYdUHtPv0Xowoc9b4PAap.jpg</td>\n",
       "      <td>2025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Très bon (7-8)</td>\n",
       "      <td>Très élevée</td>\n",
       "      <td>7.183004</td>\n",
       "      <td>Extrêmement populaire (&gt;18.1)</td>\n",
       "      <td>[-0.10469549, 0.2563884, -0.25267825, -0.28168...</td>\n",
       "      <td>0.338879</td>\n",
       "      <td>[mystérieux, mafieux, fils, adoptifs, manipule...</td>\n",
       "      <td>mystérieux mafieux fils adoptifs manipulent po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5953</th>\n",
       "      <td>341744</td>\n",
       "      <td>Truman</td>\n",
       "      <td>2015-09-24</td>\n",
       "      <td>['Drame', 'Comédie']</td>\n",
       "      <td>7.019</td>\n",
       "      <td>5.3685</td>\n",
       "      <td>Julian, un madrilène, reçoit la visite inatten...</td>\n",
       "      <td>/yzoJkuy4XqWx0YfS4WPg3umKKXX.jpg</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>Très bon (7-8)</td>\n",
       "      <td>Faible</td>\n",
       "      <td>6.666100</td>\n",
       "      <td>Très peu populaire (&lt;6.0)</td>\n",
       "      <td>[0.08660731, 0.05591605, 0.18804066, -0.209840...</td>\n",
       "      <td>0.247550</td>\n",
       "      <td>[julian, reçoit, visite, inattendue, ami, toma...</td>\n",
       "      <td>julian reçoit visite inattendue ami tomas vit ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5954</th>\n",
       "      <td>25904</td>\n",
       "      <td>Marketa Lazarova</td>\n",
       "      <td>1967-11-24</td>\n",
       "      <td>['Drame', 'Histoire']</td>\n",
       "      <td>7.766</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>Moyen Âge. En Bohême, au XIIIème siècle, chris...</td>\n",
       "      <td>/he6J26FubAydpeBal7NYUe1UeeB.jpg</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>Très bon (7-8)</td>\n",
       "      <td>Faible</td>\n",
       "      <td>7.019377</td>\n",
       "      <td>Très peu populaire (&lt;6.0)</td>\n",
       "      <td>[0.030387944, 0.37213928, 0.0035692267, 0.0441...</td>\n",
       "      <td>0.289650</td>\n",
       "      <td>[moyen, âge, bohême, siècle, christianisme, af...</td>\n",
       "      <td>moyen âge bohême siècle christianisme affronte...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>2712</td>\n",
       "      <td>Jacob</td>\n",
       "      <td>1994-12-04</td>\n",
       "      <td>['Aventure', 'Drame', 'Téléfilm']</td>\n",
       "      <td>6.556</td>\n",
       "      <td>5.3677</td>\n",
       "      <td>Jacob, fils d'Isaac et frère cadet d'Esaü, éco...</td>\n",
       "      <td>/bmwFf3fMgBKqtiST5sQ7wapeLFB.jpg</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>Bon (6-7)</td>\n",
       "      <td>Faible</td>\n",
       "      <td>6.447119</td>\n",
       "      <td>Très peu populaire (&lt;6.0)</td>\n",
       "      <td>[0.038031373, 0.40362304, 0.056222953, -0.0559...</td>\n",
       "      <td>0.267756</td>\n",
       "      <td>[jacob, fils, isaac, frère, cadet, esaü, écout...</td>\n",
       "      <td>jacob fils isaac frère cadet esaü écoute mère ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5956</th>\n",
       "      <td>323214</td>\n",
       "      <td>Alexia</td>\n",
       "      <td>2013-11-04</td>\n",
       "      <td>['Horreur', 'Thriller']</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.3676</td>\n",
       "      <td>Bien qu'Alexia, l'ex-petite amie de Franco, so...</td>\n",
       "      <td>/gJBIFfy4ej4R2ed3OfvGllkg5zw.jpg</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>Moyen (5-6)</td>\n",
       "      <td>Faible</td>\n",
       "      <td>5.900443</td>\n",
       "      <td>Très peu populaire (&lt;6.0)</td>\n",
       "      <td>[0.10010207, -0.06224392, 0.072088994, -0.1698...</td>\n",
       "      <td>0.230640</td>\n",
       "      <td>[bien, ex, petite, amie, franco, décédée, depu...</td>\n",
       "      <td>bien ex petite amie franco décédée depuis cert...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5957</th>\n",
       "      <td>102631</td>\n",
       "      <td>Dance Battle America</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>['Musique', 'Drame']</td>\n",
       "      <td>4.500</td>\n",
       "      <td>5.3672</td>\n",
       "      <td>Sean Lewis, jeune businessman condamné à des t...</td>\n",
       "      <td>/eWwGmjynnVyo0q1t972yXlHleVn.jpg</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Mauvais (3-5)</td>\n",
       "      <td>Faible</td>\n",
       "      <td>5.474865</td>\n",
       "      <td>Très peu populaire (&lt;6.0)</td>\n",
       "      <td>[0.05203962, 0.41307503, 0.013315571, -0.05203...</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>[sean, lewis, jeune, businessman, condamné, tr...</td>\n",
       "      <td>sean lewis jeune businessman condamné travaux ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5958 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                  Titre Date de sortie  \\\n",
       "0      840464  Greenland : Migration     2026-01-07   \n",
       "1     1368166     La Femme de ménage     2025-12-18   \n",
       "2     1168190        Team Démolition     2026-01-28   \n",
       "3     1084242             Zootopie 2     2025-11-26   \n",
       "4     1419406      The Shadow's Edge     2025-08-16   \n",
       "...       ...                    ...            ...   \n",
       "5953   341744                 Truman     2015-09-24   \n",
       "5954    25904       Marketa Lazarova     1967-11-24   \n",
       "5955     2712                  Jacob     1994-12-04   \n",
       "5956   323214                 Alexia     2013-11-04   \n",
       "5957   102631   Dance Battle America     2012-06-01   \n",
       "\n",
       "                                                  Genre   Note  Popularité  \\\n",
       "0           ['Aventure', 'Thriller', 'Science-Fiction']  6.567    499.8774   \n",
       "1                               ['Mystère', 'Thriller']  7.178    473.7526   \n",
       "2             ['Action', 'Comédie', 'Crime', 'Mystère']  6.802    418.4280   \n",
       "3     ['Animation', 'Comédie', 'Aventure', 'Familial...  7.600    290.4141   \n",
       "4              ['Action', 'Crime', 'Drame', 'Thriller']  7.202    262.5255   \n",
       "...                                                 ...    ...         ...   \n",
       "5953                               ['Drame', 'Comédie']  7.019      5.3685   \n",
       "5954                              ['Drame', 'Histoire']  7.766      5.3683   \n",
       "5955                  ['Aventure', 'Drame', 'Téléfilm']  6.556      5.3677   \n",
       "5956                            ['Horreur', 'Thriller']  5.400      5.3676   \n",
       "5957                               ['Musique', 'Drame']  4.500      5.3672   \n",
       "\n",
       "                                               Synopsis  \\\n",
       "0     Après l'impact dévastateur d'une comète qui a ...   \n",
       "1     En quête d’un nouveau départ, Millie accepte u...   \n",
       "2     Deux demi-frères brouillés, Jonny et James, se...   \n",
       "3     Judy Hopps et Nick Wilde explorent à nouveau Z...   \n",
       "4     Un mystérieux mafieux et ses 7 fils adoptifs m...   \n",
       "...                                                 ...   \n",
       "5953  Julian, un madrilène, reçoit la visite inatten...   \n",
       "5954  Moyen Âge. En Bohême, au XIIIème siècle, chris...   \n",
       "5955  Jacob, fils d'Isaac et frère cadet d'Esaü, éco...   \n",
       "5956  Bien qu'Alexia, l'ex-petite amie de Franco, so...   \n",
       "5957  Sean Lewis, jeune businessman condamné à des t...   \n",
       "\n",
       "                               Affiche  periode_5_ans  poids_temporel  \\\n",
       "0     /9eaHOemq2Ch5rYPJ0uXcipZUYpS.jpg           2025        1.000000   \n",
       "1     /28Dn93nuxH8PmDLUVQLCjLNVxKD.jpg           2025        0.500000   \n",
       "2     /ec0bwYUEYBhcHutx4nIShLzX7Dl.jpg           2025        1.000000   \n",
       "3     /hBI7Wrps6tDjhEzBxJgoPLhbmT1.jpg           2025        0.500000   \n",
       "4     /t1PFVsGYdUHtPv0Xowoc9b4PAap.jpg           2025        0.500000   \n",
       "...                                ...            ...             ...   \n",
       "5953  /yzoJkuy4XqWx0YfS4WPg3umKKXX.jpg           2015        0.083333   \n",
       "5954  /he6J26FubAydpeBal7NYUe1UeeB.jpg           1965        0.016667   \n",
       "5955  /bmwFf3fMgBKqtiST5sQ7wapeLFB.jpg           1990        0.030303   \n",
       "5956  /gJBIFfy4ej4R2ed3OfvGllkg5zw.jpg           2010        0.071429   \n",
       "5957  /eWwGmjynnVyo0q1t972yXlHleVn.jpg           2010        0.066667   \n",
       "\n",
       "      Note_Catégorie Pop_Catégorie  Note_Ajustée  \\\n",
       "0          Bon (6-7)   Très élevée      6.564427   \n",
       "1     Très bon (7-8)   Très élevée      7.167667   \n",
       "2          Bon (6-7)   Très élevée      6.795620   \n",
       "3     Très bon (7-8)   Très élevée      7.574757   \n",
       "4     Très bon (7-8)   Très élevée      7.183004   \n",
       "...              ...           ...           ...   \n",
       "5953  Très bon (7-8)        Faible      6.666100   \n",
       "5954  Très bon (7-8)        Faible      7.019377   \n",
       "5955       Bon (6-7)        Faible      6.447119   \n",
       "5956     Moyen (5-6)        Faible      5.900443   \n",
       "5957   Mauvais (3-5)        Faible      5.474865   \n",
       "\n",
       "                  Catégorie_Adaptée  \\\n",
       "0     Extrêmement populaire (>18.1)   \n",
       "1     Extrêmement populaire (>18.1)   \n",
       "2     Extrêmement populaire (>18.1)   \n",
       "3     Extrêmement populaire (>18.1)   \n",
       "4     Extrêmement populaire (>18.1)   \n",
       "...                             ...   \n",
       "5953      Très peu populaire (<6.0)   \n",
       "5954      Très peu populaire (<6.0)   \n",
       "5955      Très peu populaire (<6.0)   \n",
       "5956      Très peu populaire (<6.0)   \n",
       "5957      Très peu populaire (<6.0)   \n",
       "\n",
       "                                             embeddings  avg_similarity  \\\n",
       "0     [-0.027700055, 0.16916879, 0.018964056, -0.224...        0.256877   \n",
       "1     [0.10259485, -0.0042940257, -0.1724262, 0.0959...        0.343441   \n",
       "2     [0.030473026, 0.04308578, -0.07412312, -0.2527...        0.254323   \n",
       "3     [0.119301155, -0.084989265, 0.032965213, 0.155...        0.309305   \n",
       "4     [-0.10469549, 0.2563884, -0.25267825, -0.28168...        0.338879   \n",
       "...                                                 ...             ...   \n",
       "5953  [0.08660731, 0.05591605, 0.18804066, -0.209840...        0.247550   \n",
       "5954  [0.030387944, 0.37213928, 0.0035692267, 0.0441...        0.289650   \n",
       "5955  [0.038031373, 0.40362304, 0.056222953, -0.0559...        0.267756   \n",
       "5956  [0.10010207, -0.06224392, 0.072088994, -0.1698...        0.230640   \n",
       "5957  [0.05203962, 0.41307503, 0.013315571, -0.05203...        0.220339   \n",
       "\n",
       "                                         clean_synopsis  \\\n",
       "0     [après, impact, dévastateur, comète, réduit, t...   \n",
       "1     [quête, nouveau, départ, millie, accepte, post...   \n",
       "2     [deux, demi, frères, jonny, james, réunissent,...   \n",
       "3     [judy, hopps, nick, wilde, explorent, nouveau,...   \n",
       "4     [mystérieux, mafieux, fils, adoptifs, manipule...   \n",
       "...                                                 ...   \n",
       "5953  [julian, reçoit, visite, inattendue, ami, toma...   \n",
       "5954  [moyen, âge, bohême, siècle, christianisme, af...   \n",
       "5955  [jacob, fils, isaac, frère, cadet, esaü, écout...   \n",
       "5956  [bien, ex, petite, amie, franco, décédée, depu...   \n",
       "5957  [sean, lewis, jeune, businessman, condamné, tr...   \n",
       "\n",
       "                                     clean_synopsis_str  Age du film  \n",
       "0     après impact dévastateur comète réduit terre r...            0  \n",
       "1     quête nouveau départ millie accepte poste femm...            1  \n",
       "2     deux demi frères jonny james réunissent après ...            0  \n",
       "3     judy hopps nick wilde explorent nouveau zootop...            1  \n",
       "4     mystérieux mafieux fils adoptifs manipulent po...            1  \n",
       "...                                                 ...          ...  \n",
       "5953  julian reçoit visite inattendue ami tomas vit ...           11  \n",
       "5954  moyen âge bohême siècle christianisme affronte...           59  \n",
       "5955  jacob fils isaac frère cadet esaü écoute mère ...           32  \n",
       "5956  bien ex petite amie franco décédée depuis cert...           13  \n",
       "5957  sean lewis jeune businessman condamné travaux ...           14  \n",
       "\n",
       "[5958 rows x 19 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Binary encoding of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5958\n",
      "5958\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5958\n"
     ]
    }
   ],
   "source": [
    "df['Genre'] = df['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df['Genre'])\n",
    "\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop('Genre', axis=1), genre_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5958 entries, 0 to 5957\n",
      "Data columns (total 37 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   ID                  5958 non-null   int64         \n",
      " 1   Titre               5958 non-null   object        \n",
      " 2   Date de sortie      5958 non-null   datetime64[ns]\n",
      " 3   Note                5958 non-null   float64       \n",
      " 4   Popularité          5958 non-null   float64       \n",
      " 5   Synopsis            5958 non-null   object        \n",
      " 6   Affiche             5958 non-null   object        \n",
      " 7   periode_5_ans       5958 non-null   int64         \n",
      " 8   poids_temporel      5958 non-null   float64       \n",
      " 9   Note_Catégorie      5958 non-null   object        \n",
      " 10  Pop_Catégorie       5958 non-null   object        \n",
      " 11  Note_Ajustée        5958 non-null   float64       \n",
      " 12  Catégorie_Adaptée   5958 non-null   object        \n",
      " 13  embeddings          5958 non-null   object        \n",
      " 14  avg_similarity      5958 non-null   float32       \n",
      " 15  clean_synopsis      5958 non-null   object        \n",
      " 16  clean_synopsis_str  5958 non-null   object        \n",
      " 17  Age du film         5958 non-null   int32         \n",
      " 18  Action              5958 non-null   int32         \n",
      " 19  Animation           5958 non-null   int32         \n",
      " 20  Aventure            5958 non-null   int32         \n",
      " 21  Comédie             5958 non-null   int32         \n",
      " 22  Crime               5958 non-null   int32         \n",
      " 23  Documentaire        5958 non-null   int32         \n",
      " 24  Drame               5958 non-null   int32         \n",
      " 25  Familial            5958 non-null   int32         \n",
      " 26  Fantastique         5958 non-null   int32         \n",
      " 27  Guerre              5958 non-null   int32         \n",
      " 28  Histoire            5958 non-null   int32         \n",
      " 29  Horreur             5958 non-null   int32         \n",
      " 30  Musique             5958 non-null   int32         \n",
      " 31  Mystère             5958 non-null   int32         \n",
      " 32  Romance             5958 non-null   int32         \n",
      " 33  Science-Fiction     5958 non-null   int32         \n",
      " 34  Thriller            5958 non-null   int32         \n",
      " 35  Téléfilm            5958 non-null   int32         \n",
      " 36  Western             5958 non-null   int32         \n",
      "dtypes: datetime64[ns](1), float32(1), float64(4), int32(20), int64(2), object(9)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = DATA_DIR / 'df_movies_preprocess.csv'\n",
    "df.to_csv(save_path, index=False, encoding='utf-8')\n",
    "\n",
    "save_path = DATA_DIR / 'genres_binarized.csv'\n",
    "genre_df.to_csv(save_path, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
