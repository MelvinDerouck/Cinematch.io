{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin\n",
    "import os, time, sys, random\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import math\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import RegexpTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# other\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading wordpunct: Package 'wordpunct' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\derou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordpunct')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "\n",
    "#Env Perso\n",
    "df = pd.read_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\CSV\\df_movies_cleaned.csv\")\n",
    "\n",
    "# Env Vinci\n",
    "#df = pd.read_csv(r\"C:\\Users\\melvin.derouk\\Desktop\\Data formation\\Movies-Recommandations\\df_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable \"Synopsis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Work on a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de saut de ligne pour output\n",
    "def insert_newlines(string, every=80):\n",
    "    lines = []\n",
    "    for i in range(0, len(string), every):\n",
    "        lines.append(string[i:i+every])\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "À Hong Kong, J. S. Cheung est une légende de la police. Un super flic aux états de service impressionnants. Sa spécialité: le déminage, sous toutes ses formes. Lorsque un groupe de terroristes menace \n",
      "la ville avec plusieurs tonnes d'explosifs, il est décidé à prendre tous les risques. Mais ces attaques en apparence anarchiques cachent en réalité le casse du siècle.\n"
     ]
    }
   ],
   "source": [
    "doc = df.Synopsis.sample(1)\n",
    "doc = doc.values[0]\n",
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à hong kong, j. s. cheung est une légende de la police. un super flic aux états de service impressionnants. sa spécialité: le déminage, sous toutes ses formes. lorsque un groupe de terroristes menace \n",
      "la ville avec plusieurs tonnes d'explosifs, il est décidé à prendre tous les risques. mais ces attaques en apparence anarchiques cachent en réalité le casse du siècle.\n"
     ]
    }
   ],
   "source": [
    "print(insert_newlines(doc, every=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à',\n",
       " 'hong',\n",
       " 'kong',\n",
       " ',',\n",
       " 'j.',\n",
       " 's.',\n",
       " 'cheung',\n",
       " 'est',\n",
       " 'une',\n",
       " 'légende',\n",
       " 'de',\n",
       " 'la',\n",
       " 'police',\n",
       " '.',\n",
       " 'un',\n",
       " 'super',\n",
       " 'flic',\n",
       " 'aux',\n",
       " 'états',\n",
       " 'de',\n",
       " 'service',\n",
       " 'impressionnants',\n",
       " '.',\n",
       " 'sa',\n",
       " 'spécialité',\n",
       " ':',\n",
       " 'le',\n",
       " 'déminage',\n",
       " ',',\n",
       " 'sous',\n",
       " 'toutes',\n",
       " 'ses',\n",
       " 'formes',\n",
       " '.',\n",
       " 'lorsque',\n",
       " 'un',\n",
       " 'groupe',\n",
       " 'de',\n",
       " 'terroristes',\n",
       " 'menace',\n",
       " 'la',\n",
       " 'ville',\n",
       " 'avec',\n",
       " 'plusieurs',\n",
       " 'tonnes',\n",
       " \"d'explosifs\",\n",
       " ',',\n",
       " 'il',\n",
       " 'est',\n",
       " 'décidé',\n",
       " 'à',\n",
       " 'prendre',\n",
       " 'tous',\n",
       " 'les',\n",
       " 'risques',\n",
       " '.',\n",
       " 'mais',\n",
       " 'ces',\n",
       " 'attaques',\n",
       " 'en',\n",
       " 'apparence',\n",
       " 'anarchiques',\n",
       " 'cachent',\n",
       " 'en',\n",
       " 'réalité',\n",
       " 'le',\n",
       " 'casse',\n",
       " 'du',\n",
       " 'siècle',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longueur de la liste (sans les doublons)\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tokens_infos(tokens):\n",
    "    \"\"\"display info about corpus\"\"\"\n",
    "\n",
    "    print(f\"nb tokens {len(tokens)}, nb tokens uniques {len(set(tokens))}\")\n",
    "    print(tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 74, nb tokens uniques 58\n",
      "['à', 'hong', 'kong', ',', 'j', '.', 's', '.', 'cheung', 'est', 'une', 'légende', 'de', 'la', 'police', '.', 'un', 'super', 'flic', 'aux', 'états', 'de', 'service', 'impressionnants', '.', 'sa', 'spécialité', ':', 'le', 'déminage']\n"
     ]
    }
   ],
   "source": [
    "tokens = wordpunct_tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 46, nb tokens uniques 38\n",
      "['hong', 'kong', ',', '.', '.', 'cheung', 'légende', 'police', '.', 'super', 'flic', 'états', 'service', 'impressionnants', '.', 'spécialité', ':', 'déminage', ',', 'sous', 'toutes', 'formes', '.', 'lorsque', 'groupe', 'terroristes', 'menace', 'ville', 'plusieurs', 'tonnes']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 62, nb tokens uniques 54\n",
      "['à', 'hong', 'kong', 'j', 's', 'cheung', 'est', 'une', 'légende', 'de', 'la', 'police', 'un', 'super', 'flic', 'aux', 'états', 'de', 'service', 'impressionnants', 'sa', 'spécialité', 'le', 'déminage', 'sous', 'toutes', 'ses', 'formes', 'lorsque', 'un']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 34, nb tokens uniques 34\n",
      "['hong', 'kong', 'cheung', 'légende', 'police', 'super', 'flic', 'états', 'service', 'impressionnants', 'spécialité', 'déminage', 'sous', 'toutes', 'formes', 'lorsque', 'groupe', 'terroristes', 'menace', 'ville', 'plusieurs', 'tonnes', 'explosifs', 'décidé', 'prendre', 'tous', 'risques', 'attaques', 'apparence', 'anarchiques']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w for w in tokens if w not in stop_words]\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 First cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_1(doc, rejoin=False):\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    if rejoin : \n",
    "        return \" \".join(cleaned_tokens_list)\n",
    "    \n",
    "    return cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 34, nb tokens uniques 34\n",
      "['hong', 'kong', 'cheung', 'légende', 'police', 'super', 'flic', 'états', 'service', 'impressionnants', 'spécialité', 'déminage', 'sous', 'toutes', 'formes', 'lorsque', 'groupe', 'terroristes', 'menace', 'ville', 'plusieurs', 'tonnes', 'explosifs', 'décidé', 'prendre', 'tous', 'risques', 'attaques', 'apparence', 'anarchiques']\n"
     ]
    }
   ],
   "source": [
    "tokens = process_synopsis_1(doc)\n",
    "display_tokens_infos(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work on the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Leo, un lézard blasé de 74 ans, vit dans une salle de classe en Floride depuis des décennies, avec une tortue pour copain de terrarium. Aussi, quand il apprend qu'il n'a plus qu'une année à vivre, Leo décide de s'enfuir pour découvrir la vie en liberté. Mais les petits écoliers anxieux le retiennent\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = \"\".join(df.Synopsis.values)\n",
    "raw_corpus[:3_00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3831558"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 379423, nb tokens uniques 37830\n",
      "['leo', 'lézard', 'blasé', '74', 'ans', 'vit', 'salle', 'classe', 'floride', 'depuis', 'décennies', 'tortue', 'copain', 'terrarium', 'aussi', 'quand', 'apprend', 'a', 'plus', 'année', 'vivre', 'leo', 'décide', 'enfuir', 'découvrir', 'vie', 'liberté', 'petits', 'écoliers', 'anxieux']\n"
     ]
    }
   ],
   "source": [
    "corpus = process_synopsis_1(raw_corpus)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a                 3849\n",
       "plus              3258\n",
       "jeune             2082\n",
       "vie               1931\n",
       "alors             1773\n",
       "                  ... \n",
       "vauriens             1\n",
       "impressionante       1\n",
       "représentantes       1\n",
       "corley               1\n",
       "cacherait            1\n",
       "Length: 37830, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.Series(corpus).value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a        3849\n",
       "plus     3258\n",
       "jeune    2082\n",
       "vie      1931\n",
       "alors    1773\n",
       "deux     1692\n",
       "tout     1642\n",
       "va       1573\n",
       "après    1356\n",
       "faire    1332\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tobimaru          1\n",
       "bretteur          1\n",
       "luan              1\n",
       "retoucher         1\n",
       "ashburn           1\n",
       "vauriens          1\n",
       "impressionante    1\n",
       "représentantes    1\n",
       "corley            1\n",
       "cacherait         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37830.000000\n",
       "mean        10.029685\n",
       "std         53.950711\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max       3849.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 List rare tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indéterminée     1\n",
       "israéliennes     1\n",
       "brûlait          1\n",
       "guindé           1\n",
       "aimanter         1\n",
       "partira          1\n",
       "jirachi          1\n",
       "letton           1\n",
       "enrôla           1\n",
       "guadeloupe       1\n",
       "dyslexique       1\n",
       "empressant       1\n",
       "srinivasa        1\n",
       "sondes           1\n",
       "prolongements    1\n",
       "behlül           1\n",
       "trant            1\n",
       "surfaces         1\n",
       "épousera         1\n",
       "barrasser        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique words = usefull ?\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_unique_words = tmp[tmp==1]\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16319"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indéterminée',\n",
       " 'israéliennes',\n",
       " 'brûlait',\n",
       " 'guindé',\n",
       " 'aimanter',\n",
       " 'partira',\n",
       " 'jirachi',\n",
       " 'letton',\n",
       " 'enrôla',\n",
       " 'guadeloupe',\n",
       " 'dyslexique',\n",
       " 'empressant',\n",
       " 'srinivasa',\n",
       " 'sondes',\n",
       " 'prolongements',\n",
       " 'behlül',\n",
       " 'trant',\n",
       " 'surfaces',\n",
       " 'épousera',\n",
       " 'barrasser']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_unique_words = list(list_unique_words.index)\n",
    "list_unique_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_unique_words})\n",
    "tmp.to_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\CSV\\unique_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apparitions     5\n",
       "rapporte        5\n",
       "venture         5\n",
       "spécialistes    5\n",
       "révélée         5\n",
       "défilé          5\n",
       "charmeur        5\n",
       "éclaircir       5\n",
       "suivie          5\n",
       "joël            5\n",
       "clémentine      5\n",
       "options         5\n",
       "buffalo         5\n",
       "implique        5\n",
       "1917            5\n",
       "stephanie       5\n",
       "domaines        5\n",
       "kumail          5\n",
       "robes           5\n",
       "assistants      5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idem for min 5 times\n",
    "\n",
    "tmp = pd.Series(corpus).value_counts()\n",
    "list_min_5_words = tmp[tmp==5]\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_min_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apparitions',\n",
       " 'rapporte',\n",
       " 'venture',\n",
       " 'spécialistes',\n",
       " 'révélée',\n",
       " 'défilé',\n",
       " 'charmeur',\n",
       " 'éclaircir',\n",
       " 'suivie',\n",
       " 'joël',\n",
       " 'clémentine',\n",
       " 'options',\n",
       " 'buffalo',\n",
       " 'implique',\n",
       " '1917',\n",
       " 'stephanie',\n",
       " 'domaines',\n",
       " 'kumail',\n",
       " 'robes',\n",
       " 'assistants']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_min_5_words = list(list_min_5_words.index)\n",
    "list_min_5_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\"words\" : list_min_5_words})\n",
    "tmp.to_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\CSV\\min_5_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Second cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_synopsis_2(doc,\n",
    "                       rejoin=False,\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \"\"\"cf process_synopsis_1 but with list_unique_words, min_len_word, and force_is_alpha\n",
    "\n",
    "    positionnal arguments :\n",
    "    ------------------------\n",
    "    doc : str : the document (aka a text in str format) to process\n",
    "\n",
    "    opt args : \n",
    "    ------------------------\n",
    "    rejoin : bool : if True return a string else return the list of tokens\n",
    "    list_rare_words : list : a list of rare words to exclude\n",
    "    min_len_word : int : minimum lenght of a word to not exclude\n",
    "    force_is_alpha : if 1, exclude all tokens with a numeric character\n",
    "\n",
    "    return : \n",
    "    ------------------------\n",
    "    a string (if rejoin is True) or a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(alpha_tokens)\n",
    "    \n",
    "    return alpha_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 379423, nb tokens uniques 37830\n",
      "['leo', 'lézard', 'blasé', '74', 'ans', 'vit', 'salle', 'classe', 'floride', 'depuis', 'décennies', 'tortue', 'copain', 'terrarium', 'aussi', 'quand', 'apprend', 'a', 'plus', 'année', 'vivre', 'leo', 'décide', 'enfuir', 'découvrir', 'vie', 'liberté', 'petits', 'écoliers', 'anxieux']\n"
     ]
    }
   ],
   "source": [
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37830"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb tokens 350273, nb tokens uniques 21079\n",
      "['leo', 'lézard', 'blasé', 'ans', 'vit', 'salle', 'classe', 'floride', 'depuis', 'décennies', 'tortue', 'copain', 'aussi', 'quand', 'apprend', 'plus', 'année', 'vivre', 'leo', 'décide', 'enfuir', 'découvrir', 'vie', 'liberté', 'petits', 'écoliers', 'anxieux', 'retiennent', 'craignant', 'notamment']\n"
     ]
    }
   ],
   "source": [
    "#3-5 min process\n",
    "\n",
    "corpus = process_synopsis_2(raw_corpus,\n",
    "                            list_rare_words=list_unique_words,\n",
    "                            rejoin=False)\n",
    "display_tokens_infos(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21079"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stemming & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test fonction stemming\n",
    "\n",
    "def process_synopsis_3(doc,\n",
    "                       rejoin=False,\n",
    "                       lemm_or_stemm=\"stem\",\n",
    "                       list_rare_words=None,\n",
    "                       min_len_word=3,\n",
    "                       force_is_alpha=True) : \n",
    "    \n",
    "    \n",
    "    # list unique words\n",
    "    if not list_rare_words:\n",
    "        list_rare_words = []\n",
    "\n",
    "    # lower\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "    # stop words\n",
    "    cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "    # no rare tokens\n",
    "    non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "    # no more len words\n",
    "    more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "    # only alpha chars\n",
    "    if force_is_alpha : \n",
    "        alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "    else : \n",
    "        alpha_tokens = more_than_N\n",
    "\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # stem or lem\n",
    "    if lemm_or_stemm == \"lem\" : \n",
    "        trans = WordNetLemmatizer()\n",
    "        trans_text = [trans.lemmatize(i) for i in alpha_tokens]\n",
    "    else : \n",
    "        trans = FrenchStemmer()\n",
    "        trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "    #############################################################\n",
    "    #############################################################\n",
    "\n",
    "    # manage return type\n",
    "    if rejoin : \n",
    "        return \" \".join(trans_text)\n",
    "    \n",
    "    return trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test fonction lemmatize\n",
    "\n",
    "# def process_synopsis_3(doc,\n",
    "#                        rejoin=False,\n",
    "#                        lemm_or_stemm=\"lem\",\n",
    "#                        list_rare_words=None,\n",
    "#                        min_len_word=3,\n",
    "#                        force_is_alpha=True) : \n",
    "    \n",
    "    \n",
    "#     # list unique words\n",
    "#     if not list_rare_words:\n",
    "#         list_rare_words = []\n",
    "\n",
    "#     # lower\n",
    "#     doc = doc.lower().strip()\n",
    "\n",
    "#     # tokenize\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     raw_tokens_list = tokenizer.tokenize(doc)\n",
    "\n",
    "#     # stop words\n",
    "#     cleaned_tokens_list = [w for w in raw_tokens_list if w not in stop_words]\n",
    "\n",
    "#     # no rare tokens\n",
    "#     non_rare_tokens = [w for w in cleaned_tokens_list if w not in list_rare_words]\n",
    "\n",
    "#     # no more len words\n",
    "#     more_than_N = [w for w in non_rare_tokens if len(w) >= min_len_word]\n",
    "\n",
    "#     # only alpha chars\n",
    "#     if force_is_alpha : \n",
    "#         alpha_tokens = [w for w in more_than_N if w.isalpha()]\n",
    "#     else : \n",
    "#         alpha_tokens = more_than_N\n",
    "\n",
    "#     #############################################################\n",
    "#     #############################################################\n",
    "\n",
    "#     # stem or lem\n",
    "#     if lemm_or_stemm == \"lem\" : \n",
    "#         trans = nlp(' '.join(alpha_tokens))\n",
    "#         trans_text = [trans.lemma_(i) for i in alpha_tokens]\n",
    "#     else : \n",
    "#         trans = FrenchStemmer()\n",
    "#         trans_text = [trans.stem(i) for i in alpha_tokens]\n",
    "#     #############################################################\n",
    "#     #############################################################\n",
    "\n",
    "#     # manage return type\n",
    "#     if rejoin : \n",
    "#         return \" \".join(trans_text)\n",
    "    \n",
    "#     return trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247808    créatur\n",
       "226702     garçon\n",
       "297288     empêch\n",
       "95703      troubl\n",
       "162101    austral\n",
       "199540       sien\n",
       "340466        lan\n",
       "164011      coffr\n",
       "73567      biolog\n",
       "110007     chacun\n",
       "222131     combat\n",
       "330460     volont\n",
       "195766        art\n",
       "119228       tout\n",
       "232069    instabl\n",
       "76860         peu\n",
       "307485    victoir\n",
       "330031    peuvent\n",
       "192519        peu\n",
       "223030     affect\n",
       "dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_synopsis_3(raw_corpus, rejoin=False, list_rare_words=list_unique_words)\n",
    "pd.Series(corpus).sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12874"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\Notebooks\\3. Preprocessing.ipynb Cell 62\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wordcloud \u001b[39m=\u001b[39m WordCloud(width \u001b[39m=\u001b[39;49m \u001b[39m800\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                       height \u001b[39m=\u001b[39;49m \u001b[39m400\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                       background_color\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                       stopwords\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                       max_words\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\u001b[39m.\u001b[39;49mgenerate(\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(corpus))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(wordcloud, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/derou/OneDrive/Bureau/DATA/PORTFOLIO/Recommandation%20de%20films/Notebooks/3.%20Preprocessing.ipynb#Y116sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[39m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_text(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[39mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(words)\n\u001b[0;32m    622\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:453\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    451\u001b[0m     font_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_from_frequencies(\u001b[39mdict\u001b[39;49m(frequencies[:\u001b[39m2\u001b[39;49m]),\n\u001b[0;32m    454\u001b[0m                                    max_font_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight)\n\u001b[0;32m    455\u001b[0m     \u001b[39m# find font sizes\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     sizes \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout_]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\wordcloud\\wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    505\u001b[0m transposed_font \u001b[39m=\u001b[39m ImageFont\u001b[39m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m     font, orientation\u001b[39m=\u001b[39morientation)\n\u001b[0;32m    507\u001b[0m \u001b[39m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m box_size \u001b[39m=\u001b[39m draw\u001b[39m.\u001b[39;49mtextbbox((\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), word, font\u001b[39m=\u001b[39;49mtransposed_font, anchor\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    509\u001b[0m \u001b[39m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    510\u001b[0m result \u001b[39m=\u001b[39m occupancy\u001b[39m.\u001b[39msample_position(box_size[\u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    511\u001b[0m                                    box_size[\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmargin,\n\u001b[0;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(font, ImageFont\u001b[39m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mOnly supported for TrueType fonts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m embedded_color \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(width = 800,\n",
    "                      height = 400,\n",
    "                      background_color='white',\n",
    "                      stopwords=[],\n",
    "                      max_words=50).generate(\" \".join(corpus))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(doc) :\n",
    "\n",
    "    new_doc = process_synopsis_3(doc,\n",
    "                                 rejoin=False,\n",
    "                                 lemm_or_stemm=\"stem\",\n",
    "                                 list_rare_words=list_unique_words,\n",
    "                                 min_len_word=3,\n",
    "                                 force_is_alpha=True)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_synopsis'] = df.Synopsis.apply(final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on reconverti la colonne clean_synopsis_str en chaine de char\n",
    "df['clean_synopsis_str'] = df['clean_synopsis'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Titre</th>\n",
       "      <th>Date de sortie</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Note</th>\n",
       "      <th>Popularité</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Affiche</th>\n",
       "      <th>clean_synopsis</th>\n",
       "      <th>clean_synopsis_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1075794</td>\n",
       "      <td>Leo</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>['Animation', 'Comédie', 'Familial']</td>\n",
       "      <td>7.7</td>\n",
       "      <td>1828.905</td>\n",
       "      <td>Leo, un lézard blasé de 74 ans, vit dans une s...</td>\n",
       "      <td>/pD6sL4vntUOXHmuvJPPZAgvyfd9.jpg</td>\n",
       "      <td>[leo, lézard, blas, an, vit, sall, class, flor...</td>\n",
       "      <td>leo lézard blas an vit sall class florid depui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>872585</td>\n",
       "      <td>Oppenheimer</td>\n",
       "      <td>2023-07-19</td>\n",
       "      <td>['Drame', 'Histoire']</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1142.284</td>\n",
       "      <td>En 1942, convaincus que l'Allemagne nazie est ...</td>\n",
       "      <td>/boAUuJBeID7VNp4L7LNMQs8mfQS.jpg</td>\n",
       "      <td>[convaincus, allemagn, naz, train, développ, a...</td>\n",
       "      <td>convaincus allemagn naz train développ arme nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>901362</td>\n",
       "      <td>Les Trolls 3</td>\n",
       "      <td>2023-10-12</td>\n",
       "      <td>['Animation', 'Familial', 'Musique', 'Fantasti...</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1121.642</td>\n",
       "      <td>Après deux films à se tourner autour pour fina...</td>\n",
       "      <td>/r5uiTUeqoERL66fNgtPVbTViHml.jpg</td>\n",
       "      <td>[apres, deux, film, tourn, autour, final, tomb...</td>\n",
       "      <td>apres deux film tourn autour final tomb bras a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>507089</td>\n",
       "      <td>Five Nights at Freddy's</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>['Horreur', 'Mystère']</td>\n",
       "      <td>7.9</td>\n",
       "      <td>779.775</td>\n",
       "      <td>Mike, jeune homme perturbé, s’occupe de sa sœu...</td>\n",
       "      <td>/tEY81I7lpiHaLJa7AZ3O4vWXmJo.jpg</td>\n",
       "      <td>[mik, jeun, homm, perturb, occup, sœur, abby, ...</td>\n",
       "      <td>mik jeun homm perturb occup sœur abby âgé an t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670292</td>\n",
       "      <td>The Creator</td>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>['Science-Fiction', 'Action', 'Thriller']</td>\n",
       "      <td>7.1</td>\n",
       "      <td>774.741</td>\n",
       "      <td>Alors qu'une future guerre entre la race humai...</td>\n",
       "      <td>/pP1cyoXFc5Br1Sg21uORSN49yyu.jpg</td>\n",
       "      <td>[alor, futur, guerr, entre, rac, humain, intel...</td>\n",
       "      <td>alor futur guerr entre rac humain intelligent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                    Titre Date de sortie  \\\n",
       "0  1075794                      Leo     2023-11-17   \n",
       "1   872585              Oppenheimer     2023-07-19   \n",
       "2   901362             Les Trolls 3     2023-10-12   \n",
       "3   507089  Five Nights at Freddy's     2023-10-25   \n",
       "4   670292              The Creator     2023-09-27   \n",
       "\n",
       "                                               Genre  Note  Popularité  \\\n",
       "0               ['Animation', 'Comédie', 'Familial']   7.7    1828.905   \n",
       "1                              ['Drame', 'Histoire']   8.2    1142.284   \n",
       "2  ['Animation', 'Familial', 'Musique', 'Fantasti...   7.2    1121.642   \n",
       "3                             ['Horreur', 'Mystère']   7.9     779.775   \n",
       "4          ['Science-Fiction', 'Action', 'Thriller']   7.1     774.741   \n",
       "\n",
       "                                            Synopsis  \\\n",
       "0  Leo, un lézard blasé de 74 ans, vit dans une s...   \n",
       "1  En 1942, convaincus que l'Allemagne nazie est ...   \n",
       "2  Après deux films à se tourner autour pour fina...   \n",
       "3  Mike, jeune homme perturbé, s’occupe de sa sœu...   \n",
       "4  Alors qu'une future guerre entre la race humai...   \n",
       "\n",
       "                            Affiche  \\\n",
       "0  /pD6sL4vntUOXHmuvJPPZAgvyfd9.jpg   \n",
       "1  /boAUuJBeID7VNp4L7LNMQs8mfQS.jpg   \n",
       "2  /r5uiTUeqoERL66fNgtPVbTViHml.jpg   \n",
       "3  /tEY81I7lpiHaLJa7AZ3O4vWXmJo.jpg   \n",
       "4  /pP1cyoXFc5Br1Sg21uORSN49yyu.jpg   \n",
       "\n",
       "                                      clean_synopsis  \\\n",
       "0  [leo, lézard, blas, an, vit, sall, class, flor...   \n",
       "1  [convaincus, allemagn, naz, train, développ, a...   \n",
       "2  [apres, deux, film, tourn, autour, final, tomb...   \n",
       "3  [mik, jeun, homm, perturb, occup, sœur, abby, ...   \n",
       "4  [alor, futur, guerr, entre, rac, humain, intel...   \n",
       "\n",
       "                                  clean_synopsis_str  \n",
       "0  leo lézard blas an vit sall class florid depui...  \n",
       "1  convaincus allemagn naz train développ arme nu...  \n",
       "2  apres deux film tourn autour final tomb bras a...  \n",
       "3  mik jeun homm perturb occup sœur abby âgé an t...  \n",
       "4  alor futur guerr entre rac humain intelligent ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Date de sortie to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date de sortie'] = pd.to_datetime(df['Date de sortie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Age of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = datetime.now().year\n",
    "df['Age du film'] = current_year - df['Date de sortie'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Binary encoding of genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9182\n",
      "9182\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, str)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9182\n"
     ]
    }
   ],
   "source": [
    "df['Genre'] = df['Genre'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "print(df['Genre'].apply(lambda x: isinstance(x, list)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(df['Genre'])\n",
    "\n",
    "genre_df = pd.DataFrame(genre_binarized, columns=mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop('Genre', axis=1), genre_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9182 entries, 0 to 9181\n",
      "Data columns (total 29 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   ID                  9182 non-null   int64         \n",
      " 1   Titre               9182 non-null   object        \n",
      " 2   Date de sortie      9182 non-null   datetime64[ns]\n",
      " 3   Note                9182 non-null   float64       \n",
      " 4   Popularité          9182 non-null   float64       \n",
      " 5   Synopsis            9182 non-null   object        \n",
      " 6   Affiche             9182 non-null   object        \n",
      " 7   clean_synopsis      9182 non-null   object        \n",
      " 8   clean_synopsis_str  9182 non-null   object        \n",
      " 9   Age du film         9182 non-null   int64         \n",
      " 10  Action              9182 non-null   int32         \n",
      " 11  Animation           9182 non-null   int32         \n",
      " 12  Aventure            9182 non-null   int32         \n",
      " 13  Comédie             9182 non-null   int32         \n",
      " 14  Crime               9182 non-null   int32         \n",
      " 15  Documentaire        9182 non-null   int32         \n",
      " 16  Drame               9182 non-null   int32         \n",
      " 17  Familial            9182 non-null   int32         \n",
      " 18  Fantastique         9182 non-null   int32         \n",
      " 19  Guerre              9182 non-null   int32         \n",
      " 20  Histoire            9182 non-null   int32         \n",
      " 21  Horreur             9182 non-null   int32         \n",
      " 22  Musique             9182 non-null   int32         \n",
      " 23  Mystère             9182 non-null   int32         \n",
      " 24  Romance             9182 non-null   int32         \n",
      " 25  Science-Fiction     9182 non-null   int32         \n",
      " 26  Thriller            9182 non-null   int32         \n",
      " 27  Téléfilm            9182 non-null   int32         \n",
      " 28  Western             9182 non-null   int32         \n",
      "dtypes: datetime64[ns](1), float64(2), int32(19), int64(2), object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final['tags'] = df_final['clean_synopsis_str']+df_final['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del_columns = ['Genre', 'clean_synopsis', 'clean_synopsis_str']\n",
    "\n",
    "# df_final.drop(columns=del_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Titre', 'Date de sortie', 'Genre', 'Note', 'Popularité',\n",
       "       'Synopsis', 'Affiche', 'clean_synopsis', 'clean_synopsis_str',\n",
       "       'Age du film', 'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = df_final[['ID', 'Titre', 'Date de sortie', 'Age du film', 'Note', 'Popularité',\n",
    "#                      'Synopsis', 'tags', 'Affiche']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\CSV\\df_movies_preprocess.csv\", index=False)\n",
    "genre_df.to_csv(r\"C:\\Users\\derou\\OneDrive\\Bureau\\DATA\\PORTFOLIO\\Recommandation de films\\CSV\\genres_binarized.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
